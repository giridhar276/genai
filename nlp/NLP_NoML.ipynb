{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "qK871TD5sWZ7",
      "metadata": {
        "id": "qK871TD5sWZ7"
      },
      "source": [
        "# NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WWjriMkb-13c",
      "metadata": {
        "id": "WWjriMkb-13c"
      },
      "source": [
        "**Text cleaning & normalization**\n",
        "\n",
        "re (built-in regex), string, unicodedata\n",
        "\n",
        "ftfy (fixes broken unicode)\n",
        "\n",
        "unidecode (accent → ASCII)\n",
        "\n",
        "emoji / emot (detect/remove/describe emojis)\n",
        "\n",
        "clean-text, neattext (common cleaning ops)\n",
        "\n",
        "**Tokenization & segmentation (rule-based)**\n",
        "\n",
        "sacremoses (Moses tokenizer, rule-based)\n",
        "\n",
        "nltk tokenizers (Punkt/simpler word tokenizers; no training needed)\n",
        "\n",
        "spaCy tokenizer without loading a model (pure rule rules for many langs)\n",
        "\n",
        "**Stemming & lemmatization (non-ML)**\n",
        "\n",
        "nltk.stem (Porter, Snowball, Lancaster)\n",
        "\n",
        "snowballstemmer (standalone)\n",
        "\n",
        "simplemma (dictionary-based lemmatizer)\n",
        "\n",
        "**Keyword/phrase matching & string search**\n",
        "\n",
        "flashtext (very fast exact keyword extraction)\n",
        "\n",
        "pyahocorasick (multi-pattern search; Aho-Corasick automaton)\n",
        "\n",
        "rapidfuzz / python-Levenshtein (fuzzy matching, edit distances)\n",
        "\n",
        "regex (advanced PCRE-like features)\n",
        "\n",
        "**Rule-based pattern extraction**\n",
        "\n",
        "spaCy’s Matcher/PhraseMatcher (can use without ML models)\n",
        "\n",
        "dateparser (recognize/normalize dates)\n",
        "\n",
        "wordfreq (word frequencies, useful for heuristics)\n",
        "\n",
        "textstat (readability metrics)\n",
        "\n",
        "**Spell-check & correction (non-ML)**\n",
        "\n",
        "pyenchant (dictionary-based spellcheck)\n",
        "\n",
        "symspellpy (very fast SymSpell corrections)\n",
        "\n",
        "**Sentiment & simple scoring (lexicon-based)**\n",
        "\n",
        "vaderSentiment (rule/lexicon-based sentiment, great for social text)\n",
        "\n",
        "textblob (can use the default Pattern analyzer = lexicon-based)\n",
        "\n",
        "**Transliteration & Indic text utilities (mostly rule-based)**\n",
        "\n",
        "indic-transliteration (script ↔ script mapping)\n",
        "\n",
        "indic-nlp-library (normalizers/tokenizers for Indic)\n",
        "\n",
        "pykakasi (JP kana/kanji → romaji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aiyZcNqf-1dP",
      "metadata": {
        "id": "aiyZcNqf-1dP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "y-xgREVgsWZ8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y-xgREVgsWZ8",
        "outputId": "9f27f68e-43d7-47d8-b355-44dcb39cfc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting neattext\n",
            "  Downloading neattext-0.1.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m741.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171031 sha256=598ab8e3df3a0342532ec1e15045702f7c99d348e118c9dd32de888a908163d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/8c/e0/294d2e4ea0e55792bfc99b6b263e4a0511443da7b69af67688\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, unidecode, neattext, ftfy, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.3.1 neattext-0.1.3 unidecode-1.4.0\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "Requirement already satisfied: snowballstemmer in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Collecting simplemma\n",
            "  Downloading simplemma-1.1.2-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading simplemma-1.1.2-py3-none-any.whl (67.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplemma\n",
            "Successfully installed simplemma-1.1.2\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading pyahocorasick-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9300 sha256=cca5cf2811544ada1ab91146f7d2455d624a179bf4625784bf396e8be497245f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/24/da/4d994d7a27cfc73a4e513a669fbeec4a71f871fe245a81977f\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext, rapidfuzz, pyahocorasick\n",
            "Successfully installed flashtext-2.7 pyahocorasick-2.2.0 rapidfuzz-3.14.1\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.10-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (5.3.1)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (3.5.0)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (1.1.1)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (4.67.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.3.1)\n",
            "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textstat-0.7.10-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, locate, textstat, dateparser, wordfreq\n",
            "Successfully installed dateparser-1.2.2 locate-1.1.1 pyphen-0.17.2 textstat-0.7.10 wordfreq-3.1.1\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Downloading pyenchant-3.3.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyenchant, editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.6 pyenchant-3.3.0 symspellpy-6.9.0\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.8.3)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting indic-transliteration\n",
            "  Downloading indic_transliteration-2.3.75-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pykakasi\n",
            "  Downloading pykakasi-2.3.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting backports.functools-lru-cache (from indic-transliteration)\n",
            "  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (2024.11.6)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (0.17.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from indic-transliteration) (0.10.2)\n",
            "Collecting roman (from indic-transliteration)\n",
            "  Downloading roman-5.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.0.2)\n",
            "Collecting jaconv (from pykakasi)\n",
            "  Downloading jaconv-0.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->pykakasi) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (8.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (4.15.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic-transliteration) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.4)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.8.3)\n",
            "Downloading indic_transliteration-2.3.75-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pykakasi-2.3.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading roman-5.1-py3-none-any.whl (5.8 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jaconv\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.4.0-py3-none-any.whl size=18228 sha256=00d0c7f0bc03c6e56c10a6f834d06a5e994c052dc2e8f3d55d20b5da972b950a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/97/53/1f827ebf916b899520ce663227497206525c432be72cc29265\n",
            "Successfully built jaconv\n",
            "Installing collected packages: morfessor, jaconv, roman, deprecated, backports.functools-lru-cache, pykakasi, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-transliteration, indic-nlp-library\n",
            "Successfully installed backports.functools-lru-cache-2.0.0 deprecated-1.2.18 indic-nlp-library-0.92 indic-transliteration-2.3.75 jaconv-0.4.0 morfessor-2.0.6 pykakasi-2.3.0 roman-5.1 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "56fce2a7e94f4ab2885ea7d87846870f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Setup cell ready. Uncomment the lines you need and run.\n"
          ]
        }
      ],
      "source": [
        "# --- Setup: install third‑party libraries (run as needed) ---\n",
        "# You can comment out any lines you don't need.\n",
        "\n",
        "# Core text cleaning\n",
        "!pip install ftfy unidecode emoji clean-text neattext regex\n",
        "\n",
        "# Tokenization & segmentation\n",
        "!pip install sacremoses nltk spacy\n",
        "\n",
        "# Morphology / lemmatization\n",
        "!pip install snowballstemmer simplemma\n",
        "\n",
        "# Keyword search / fuzzy\n",
        "!pip install flashtext pyahocorasick rapidfuzz\n",
        "\n",
        "# Rule-based extractors / utils\n",
        "!pip install dateparser wordfreq textstat\n",
        "\n",
        "# Spell-check\n",
        "!pip install pyenchant symspellpy\n",
        "\n",
        "# Sentiment (lexicon-based) & simple NLP\n",
        "!pip install vaderSentiment textblob\n",
        "\n",
        "# Indic tools & transliteration\n",
        "!pip install indic-transliteration indic-nlp-library pykakasi\n",
        "\n",
        "# One‑time NLTK downloads (for tokenizers / VADER lexicon / etc.)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # for newer nltk versions\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# (Optional) spaCy: download a small model if you later want ML features\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k2V249hasWZ9",
      "metadata": {
        "id": "k2V249hasWZ9"
      },
      "source": [
        "## re (built‑in regular expressions)\n",
        "\n",
        "Classic regex tasks: tokenization, extraction, substitution, named groups, and lookaheads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ez_j9WdOsWZ9",
      "metadata": {
        "id": "ez_j9WdOsWZ9"
      },
      "source": [
        "### Word tokenization (simple regex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "D0U4dqpVsWZ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0U4dqpVsWZ9",
        "outputId": "da8371e1-9ab0-4245-bcfa-1023cb6f2ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AI', 'A', 'I', 'is', 'AMAZING', 'Visit', 'us', 'at', 'ai', 'example', 'com']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"AI (A.I.) is AMAZING!!! Visit us at ai@example.com\"\n",
        "tokens = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q80mrm93sWZ9",
      "metadata": {
        "id": "Q80mrm93sWZ9"
      },
      "source": [
        "### Extract emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Zd5d4HF0sWZ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd5d4HF0sWZ9",
        "outputId": "71b407a5-079d-49b0-e480-23c30c952a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alice@test.org', 'bob.smith@company.co.in']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Contact: alice@test.org, bob.smith@company.co.in\"\n",
        "emails = re.findall(r\"[\\w\\.-]+@[\\w\\.-]+\", text)\n",
        "print(emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JgmeOFfjsWZ9",
      "metadata": {
        "id": "JgmeOFfjsWZ9"
      },
      "source": [
        "### Substitute multiple spaces / punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "-iLYqbbIsWZ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iLYqbbIsWZ9",
        "outputId": "7998a8df-a2bd-43dd-fa09-c56b36460442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! world!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Hello,,,   world!!\"\n",
        "clean = re.sub(r\"[!,]+\", \"!\", text)\n",
        "clean = re.sub(r\"\\s+\", \" \", clean).strip()\n",
        "print(clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cI9S_bLdsWZ9",
      "metadata": {
        "id": "cI9S_bLdsWZ9"
      },
      "source": [
        "### Named groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "FVdWtGFgsWZ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVdWtGFgsWZ9",
        "outputId": "fc7dd40a-ebfe-4403-a658-4c72decf8df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user': 'jane', 'evt': 'LOGIN'}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "log = \"USER=jane EVENT=LOGIN TS=2025-09-15\"\n",
        "m = re.search(r\"USER=(?P<user>\\w+)\\s+EVENT=(?P<evt>\\w+)\", log)\n",
        "print(m.groupdict())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qSqWsEQ7sWZ9",
      "metadata": {
        "id": "qSqWsEQ7sWZ9"
      },
      "source": [
        "### Positive lookahead (extract words before '!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aYIR6HiQsWZ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYIR6HiQsWZ9",
        "outputId": "060ae998-cc14-46b1-a80f-3bc1d30ec046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Great', 'Awesome']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Great! Awesome! Meh.\"\n",
        "hits = re.findall(r\"\\b\\w+(?=!)\", text)  # words followed by '!'\n",
        "print(hits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJz_YnufsWZ9",
      "metadata": {
        "id": "yJz_YnufsWZ9"
      },
      "source": [
        "## string (built‑in)\n",
        "\n",
        "Utilities for punctuation, digits, ascii letters, templates, and simple transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nFLSPz7PsWZ9",
      "metadata": {
        "id": "nFLSPz7PsWZ9"
      },
      "source": [
        "### Remove punctuation using string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "Fm19qNsbsWZ-",
      "metadata": {
        "id": "Fm19qNsbsWZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ea7ae8-cd63-4ddf-f991-5da3a32c8a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world NLP\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "text = \"Hello, world! #NLP\"\n",
        "no_punct = \"\".join(ch for ch in text if ch not in string.punctuation)\n",
        "print(no_punct)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_FSG52rXsWZ-",
      "metadata": {
        "id": "_FSG52rXsWZ-"
      },
      "source": [
        "### Check character classes (ascii letters / digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "yKge0nXpsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKge0nXpsWZ-",
        "outputId": "e4b62542-8092-4e21-b882-172a18b13556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "s = \"A1b2C3\"\n",
        "print(all(ch in string.ascii_letters+string.digits for ch in s))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UNopFo-bsWZ-",
      "metadata": {
        "id": "UNopFo-bsWZ-"
      },
      "source": [
        "### Using string.Template for safe substitution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "pZb8qv9WsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZb8qv9WsWZ-",
        "outputId": "6b258b41-e819-41a2-eb68-b79badaea98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Giri! Today is Monday.\n"
          ]
        }
      ],
      "source": [
        "from string import Template\n",
        "t = Template(\"Hello, $name! Today is $day.\")\n",
        "print(t.substitute(name=\"Giri\", day=\"Monday\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uo60VqgcsWZ-",
      "metadata": {
        "id": "Uo60VqgcsWZ-"
      },
      "source": [
        "### Capitalize words with string.capwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "T0R65tHCsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0R65tHCsWZ-",
        "outputId": "d893a87d-fa86-4c08-f93e-c7edcea93767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello From Hyderabad\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "text = \"hello from hyderabad\"\n",
        "print(string.capwords(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7yG-VaPBsWZ-",
      "metadata": {
        "id": "7yG-VaPBsWZ-"
      },
      "source": [
        "### Custom translation table (remove digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eEuI3sb-sWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEuI3sb-sWZ-",
        "outputId": "303ed140-78b8-400d-f4aa-84b56709f15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a b c\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "text = \"a1 b2 c3\"\n",
        "tbl = str.maketrans(\"\", \"\", string.digits)\n",
        "print(text.translate(tbl))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VzhMYmOlsWZ-",
      "metadata": {
        "id": "VzhMYmOlsWZ-"
      },
      "source": [
        "## unicodedata (built‑in)\n",
        "\n",
        "Normalize, inspect categories, names, and numeric values of Unicode characters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rW0lQnqvsWZ-",
      "metadata": {
        "id": "rW0lQnqvsWZ-"
      },
      "source": [
        "### NFC vs NFD normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "l1qGho_XsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1qGho_XsWZ-",
        "outputId": "4ada48b8-e536-4071-ebad-6f7ff4c01a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "café | café | café\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "s = \"café\"\n",
        "nfd = unicodedata.normalize(\"NFD\", s)\n",
        "nfc = unicodedata.normalize(\"NFC\", nfd)\n",
        "print(s, nfd, nfc, sep=\" | \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UCO2I4MLsWZ-",
      "metadata": {
        "id": "UCO2I4MLsWZ-"
      },
      "source": [
        "### Character categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6bcBInKLsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bcBInKLsWZ-",
        "outputId": "2db3842e-7b1e-4259-ef9d-0b9e38eb37f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'Lu'), ('é', 'Ll'), ('—', 'Pd'), ('🙂', 'So')]\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "chars = ['A', 'é', '—', '🙂']\n",
        "print([ (c, unicodedata.category(c)) for c in chars ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gjUDK3gosWZ-",
      "metadata": {
        "id": "gjUDK3gosWZ-"
      },
      "source": [
        "### Character names (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "vCvTAFfWsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCvTAFfWsWZ-",
        "outputId": "a23903c6-f8bb-44bd-b054-2bf53e87b223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', 'LATIN CAPITAL LETTER A'), ('é', 'LATIN SMALL LETTER E WITH ACUTE'), ('—', 'EM DASH'), ('🙂', 'SLIGHTLY SMILING FACE')]\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "chars = ['A', 'é', '—', '🙂']\n",
        "print([ (c, unicodedata.name(c, 'UNKNOWN')) for c in chars ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RFD9rVEAsWZ-",
      "metadata": {
        "id": "RFD9rVEAsWZ-"
      },
      "source": [
        "### Strip combining marks (decompose then drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "JdP7CmxJsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdP7CmxJsWZ-",
        "outputId": "d1fc4e3d-4aaa-47e5-e811-e701ae848908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cafe\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "s = \"café\"\n",
        "nfd = unicodedata.normalize(\"NFD\", s)\n",
        "stripped = \"\".join(ch for ch in nfd if unicodedata.category(ch) != 'Mn')\n",
        "print(stripped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_-4ogfLgsWZ-",
      "metadata": {
        "id": "_-4ogfLgsWZ-"
      },
      "source": [
        "### Numeric values in unicode (e.g., Roman numerals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5jwpNwJOsWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jwpNwJOsWZ-",
        "outputId": "3d1ed3d3-0ffc-45ac-fdec-60aa9d61c82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.0, 5.0, 5.0]\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "chars = ['Ⅳ', 'Ⅴ', '５']  # roman 4, 5, fullwidth 5\n",
        "print([ unicodedata.numeric(c, None) for c in chars ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4bzdNGsWZ-",
      "metadata": {
        "id": "3a4bzdNGsWZ-"
      },
      "source": [
        "## ftfy\n",
        "\n",
        "Fixes broken Unicode text (mojibake, curly quotes, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n1M3I5iAsWZ-",
      "metadata": {
        "id": "n1M3I5iAsWZ-"
      },
      "source": [
        "### Fix mojibake / smart quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "-DjVgztpuc-M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DjVgztpuc-M",
        "outputId": "69361bfa-b7bc-49aa-b395-ef1dcf23dba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aPYx2Lv3sWZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPYx2Lv3sWZ-",
        "outputId": "a8dab187-3194-4b9b-d099-7cdf1c5543db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This — is \"broken\" text…\n",
            "Install ftfy to run this example.\n"
          ]
        }
      ],
      "source": [
        "from ftfy import fix_text\n",
        "text = \"This â€” is \\\"broken\\\" text…\"\n",
        "print(fix_text(text))\n",
        "print(\"Install ftfy to run this example.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ajnBbplAsWZ_",
      "metadata": {
        "id": "ajnBbplAsWZ_"
      },
      "source": [
        "### Normalize weird spacing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "PCN_SC6qsWaC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCN_SC6qsWaC",
        "outputId": "a35efe8a-72a3-46ed-9405-597b66c1e076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world\n"
          ]
        }
      ],
      "source": [
        "from ftfy import fix_text\n",
        "print(fix_text(\"Hello\\u00A0world\"))  # NBSP -> space\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XBjQ6KmrsWaD",
      "metadata": {
        "id": "XBjQ6KmrsWaD"
      },
      "source": [
        "### Repair ligatures & punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "nz4i79EwsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz4i79EwsWaD",
        "outputId": "4f6c7b6f-68a9-4ef1-ea59-aaa362d63a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "offifice — fifles\n"
          ]
        }
      ],
      "source": [
        "from ftfy import fix_text\n",
        "print(fix_text(\"offiﬁce — fiﬂes\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nClAyW1usWaD",
      "metadata": {
        "id": "nClAyW1usWaD"
      },
      "source": [
        "### Decode mixed encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7Xj4BiAvsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xj4BiAvsWaD",
        "outputId": "16a1c7ea-9781-430a-e238-542b9f67b7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "café -> café\n"
          ]
        }
      ],
      "source": [
        "from ftfy import fix_text\n",
        "print(fix_text('cafÃ© -> café'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oAJlNtv5sWaD",
      "metadata": {
        "id": "oAJlNtv5sWaD"
      },
      "source": [
        "### Canonicalize quotes/dashes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5E9JBWDEsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E9JBWDEsWaD",
        "outputId": "847ffa0f-59de-4178-dd67-d05b1b16d85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He said, 'hello' -- ok?\n"
          ]
        }
      ],
      "source": [
        "from ftfy import fix_text\n",
        "print(fix_text(\"He said, ‘hello’ -- ok?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hZZl1VwrsWaD",
      "metadata": {
        "id": "hZZl1VwrsWaD"
      },
      "source": [
        "## unidecode\n",
        "\n",
        "ASCII transliteration for non‑ASCII characters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1YbxlI0QsWaD",
      "metadata": {
        "id": "1YbxlI0QsWaD"
      },
      "source": [
        "### Accented Latin → ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "JKF7Vzc1sWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKF7Vzc1sWaD",
        "outputId": "75a3cb8c-7d94-47ec-8ecd-0a60cdc8a202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cafe naive jalapeno\n"
          ]
        }
      ],
      "source": [
        "from unidecode import unidecode\n",
        "print(unidecode(\"café naïve jalapeño\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xpvv3ckCsWaD",
      "metadata": {
        "id": "xpvv3ckCsWaD"
      },
      "source": [
        "### Greek → ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "CIqKdTHtsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIqKdTHtsWaD",
        "outputId": "a64a1962-efcd-425b-b83a-92209f24cd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Athena (Athens)\n"
          ]
        }
      ],
      "source": [
        "from unidecode import unidecode\n",
        "print(unidecode(\"Αθήνα (Athens)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dLlRwySsWaD",
      "metadata": {
        "id": "7dLlRwySsWaD"
      },
      "source": [
        "### Cyrillic → ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "YnQK_W5MsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnQK_W5MsWaD",
        "outputId": "6e9deb94-8e6b-4ca4-c305-87da90f5853c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Privet mir\n"
          ]
        }
      ],
      "source": [
        "from unidecode import unidecode\n",
        "print(unidecode(\"Привет мир\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cKucfiwksWaD",
      "metadata": {
        "id": "cKucfiwksWaD"
      },
      "source": [
        "### Han/Kanji → phonetic ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "mPyZueXTsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPyZueXTsWaD",
        "outputId": "cb71ccc8-49ab-46ca-cc8f-ecbfe449fb98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dong Jing \n"
          ]
        }
      ],
      "source": [
        "from unidecode import unidecode\n",
        "print(unidecode(\"東京\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EUMRJfuMsWaD",
      "metadata": {
        "id": "EUMRJfuMsWaD"
      },
      "source": [
        "### Emoji fallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Ql9uERfssWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql9uERfssWaD",
        "outputId": "eaaad18b-a551-4634-8627-49ad334b4888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I  NLP\n"
          ]
        }
      ],
      "source": [
        "from unidecode import unidecode\n",
        "print(unidecode(\"I ❤️ NLP\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81eiXMVlsWaD",
      "metadata": {
        "id": "81eiXMVlsWaD"
      },
      "source": [
        "## emoji\n",
        "\n",
        "Detect, demojize (emoji → :shortcode:), emojize (:shortcode: → emoji)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_bd9k0fDsWaD",
      "metadata": {
        "id": "_bd9k0fDsWaD"
      },
      "source": [
        "### Find emojis in text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "EOzzt4vTsWaD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOzzt4vTsWaD",
        "outputId": "a4e3b9b1-b96f-4961-ebdd-61df1b4c14a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['👍', '🔥']\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "text = \"Great job! 👍🔥\"\n",
        "print([ch for ch in text if ch in emoji.EMOJI_DATA])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TVZE4bYYsWaE",
      "metadata": {
        "id": "TVZE4bYYsWaE"
      },
      "source": [
        "### Demojize to :shortcodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3oSXz4hasWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSXz4hasWaE",
        "outputId": "93b7cbcb-2dcc-4347-d624-0d8f846eddaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I :red_heart: NLP\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "print(emoji.demojize(\"I ❤️ NLP\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LJA4g7v4sWaE",
      "metadata": {
        "id": "LJA4g7v4sWaE"
      },
      "source": [
        "### Emojize from :shortcodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "jNVJOvyysWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNVJOvyysWaE",
        "outputId": "c212443e-6ed2-4a27-d192-d836d3b86599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is 🔥\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "print(emoji.emojize(\"Python is :fire:\", language='alias'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gp8Y_5Z-sWaE",
      "metadata": {
        "id": "Gp8Y_5Z-sWaE"
      },
      "source": [
        "### Replace emojis with tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "Lc4K5aoDsWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc4K5aoDsWaE",
        "outputId": "e46c3b5d-807f-4519-f488-d4fe0e5996f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So happy <EMOJI>\n"
          ]
        }
      ],
      "source": [
        "import emoji, re\n",
        "text = \"So happy 😊\"\n",
        "print(re.sub(r\"[\\U0001F600-\\U0001F64F]\", \"<EMOJI>\", text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RgHSOPZnsWaE",
      "metadata": {
        "id": "RgHSOPZnsWaE"
      },
      "source": [
        "### Count emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "y1dwzn06sWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1dwzn06sWaE",
        "outputId": "5df84000-6009-47af-bab6-703edf919dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "text = \"wow 😮😮😮\"\n",
        "print(sum(ch in emoji.EMOJI_DATA for ch in text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AnlfrbwbsWaE",
      "metadata": {
        "id": "AnlfrbwbsWaE"
      },
      "source": [
        "## clean-text\n",
        "\n",
        "One‑liner cleaning utilities via `cleantext.clean`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7pUrhpqGsWaE",
      "metadata": {
        "id": "7pUrhpqGsWaE"
      },
      "source": [
        "### Basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "IJNbc0RNsWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJNbc0RNsWaE",
        "outputId": "9091d5fb-1abb-4b03-d086-ebb934210326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello visit httpsxy nlp\n"
          ]
        }
      ],
      "source": [
        "from cleantext import clean\n",
        "s = \"  Hello!!! Visit https://x.y  \\n \\t #NLP \"\n",
        "print(clean(s, lower=True, no_urls=True, no_punct=True, no_line_breaks=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-qx42_IwsWaE",
      "metadata": {
        "id": "-qx42_IwsWaE"
      },
      "source": [
        "### Strip emojis & accents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "tpdq2oHYsWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpdq2oHYsWaE",
        "outputId": "24635d57-40e7-4280-951f-c45d461b3bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cafe\n"
          ]
        }
      ],
      "source": [
        "from cleantext import clean\n",
        "print(clean(\"Café 😊\", no_emoji=True, fix_unicode=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIRk-5eHsWaE",
      "metadata": {
        "id": "nIRk-5eHsWaE"
      },
      "source": [
        "### Keep case but remove digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "Y1L4VaOMsWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1L4VaOMsWaE",
        "outputId": "5959af5f-326b-4737-ba95-3a3a86b9324f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1 B2 C3\n"
          ]
        }
      ],
      "source": [
        "from cleantext import clean\n",
        "print(clean(\"A1 B2 C3\", lower=False, no_numbers=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MpemL_c4sWaE",
      "metadata": {
        "id": "MpemL_c4sWaE"
      },
      "source": [
        "### Replace currency symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "h7zfmqU8sWaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7zfmqU8sWaE",
        "outputId": "b494c6fa-438b-471f-9d64-967eb0b1f992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: rs100 or $5\n"
          ]
        }
      ],
      "source": [
        "from cleantext import clean\n",
        "#print(clean(\"Price: ₹100 or $5\", replace_with_currency=\"CUR\"))\n",
        "print(clean(\"Price: ₹100 or $5\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TiCEh4y3sWaE",
      "metadata": {
        "id": "TiCEh4y3sWaE"
      },
      "source": [
        "### Custom replacements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ZWDD3fIlsWaE",
      "metadata": {
        "id": "ZWDD3fIlsWaE"
      },
      "outputs": [],
      "source": [
        "from cleantext import clean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zP-YD8olsWaE",
      "metadata": {
        "id": "zP-YD8olsWaE"
      },
      "source": [
        "## neattext\n",
        "\n",
        "Convenient text cleaning helpers with chainable API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IULi18cAsWaF",
      "metadata": {
        "id": "IULi18cAsWaF"
      },
      "source": [
        "### Remove special chars and digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "A2tdrFIZsWaF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2tdrFIZsWaF",
        "outputId": "f720ea94-ea82-42c0-e080-68acd789ea77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a b c\n"
          ]
        }
      ],
      "source": [
        "import neattext.functions as nfx\n",
        "s = \"a1! b2? c3.\"\n",
        "print(nfx.remove_special_characters(nfx.remove_numbers(s)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vfgztRgrw2kH",
      "metadata": {
        "id": "vfgztRgrw2kH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "-Zhn5iYvsWaF",
      "metadata": {
        "id": "-Zhn5iYvsWaF"
      },
      "source": [
        "### Extract emails & phones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "dM2vu2GEsWaF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM2vu2GEsWaF",
        "outputId": "894e2c7c-9f3c-4b63-d155-99397b5e3acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a@b.com'] ['+91-90000-12345']\n"
          ]
        }
      ],
      "source": [
        "import neattext.functions as nfx\n",
        "s = \"Email me at a@b.com or +91-90000-12345\"\n",
        "print(nfx.extract_emails(s), nfx.extract_phone_numbers(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HnIH2fi0sWaF",
      "metadata": {
        "id": "HnIH2fi0sWaF"
      },
      "source": [
        "### Normalize whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "mucg-2s5sWaF",
      "metadata": {
        "id": "mucg-2s5sWaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d4eae1-5951-4f3c-f975-3c178f6aaaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method TextFrame.word_tokens of TextFrame(text=\"NLP is fun.\")>\n"
          ]
        }
      ],
      "source": [
        "import neattext as nt\n",
        "doc = nt.TextFrame(text=\"NLP is fun.\")\n",
        "print(doc.word_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pchuN5fRsWaF",
      "metadata": {
        "id": "pchuN5fRsWaF"
      },
      "source": [
        "## sacremoses (Moses tokenizer)\n",
        "\n",
        "Rule‑based tokenization/detokenization (Moses)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vO9lR6cqsWaF",
      "metadata": {
        "id": "vO9lR6cqsWaF"
      },
      "source": [
        "### Basic tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "UFs1PFQ9sWaF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFs1PFQ9sWaF",
        "outputId": "5a6830aa-85f4-4f6c-a431-cc94de8ff8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!']\n"
          ]
        }
      ],
      "source": [
        "from sacremoses import MosesTokenizer\n",
        "mt = MosesTokenizer(lang='en')\n",
        "print(mt.tokenize(\"Hello, world!\", return_str=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kYJD4V_nsWaF",
      "metadata": {
        "id": "kYJD4V_nsWaF"
      },
      "source": [
        "### Return as string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "-n3OYcY0sWaF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n3OYcY0sWaF",
        "outputId": "7765f145-af76-4065-cd93-659ade5c0fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Don &apos;t split contractions .\n"
          ]
        }
      ],
      "source": [
        "from sacremoses import MosesTokenizer\n",
        "mt = MosesTokenizer(lang='en')\n",
        "print(mt.tokenize(\"Don't split contractions.\", return_str=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ov0EL66bsWaF",
      "metadata": {
        "id": "ov0EL66bsWaF"
      },
      "source": [
        "### Detokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "_1_P7n0BsWaF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1_P7n0BsWaF",
        "outputId": "92162e3f-6b43-4758-be22-9842c968b822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world!\n"
          ]
        }
      ],
      "source": [
        "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
        "mt = MosesTokenizer(lang='en')\n",
        "md = MosesDetokenizer(lang='en')\n",
        "toks = mt.tokenize(\"Hello, world!\")\n",
        "print(md.detokenize(toks))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606n3E-csWaF",
      "metadata": {
        "id": "606n3E-csWaF"
      },
      "source": [
        "### Escape XML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "IUM9g7u9sWaF",
      "metadata": {
        "id": "IUM9g7u9sWaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f603a9c0-d242-4a8b-b283-4e9959d887a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 &lt; 6 &amp; 7 &gt; 3\n"
          ]
        }
      ],
      "source": [
        "from sacremoses import MosesTokenizer\n",
        "mt = MosesTokenizer()\n",
        "print(mt.escape_xml(\"5 < 6 & 7 > 3\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UzTYiI5fsWaF",
      "metadata": {
        "id": "UzTYiI5fsWaF"
      },
      "source": [
        "### Non‑English example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "mugRz2wEsWaF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mugRz2wEsWaF",
        "outputId": "0e2d30c9-3e74-4ca9-dc5f-756afe27f391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Das', 'ist', 'großartig', '!']\n"
          ]
        }
      ],
      "source": [
        "from sacremoses import MosesTokenizer\n",
        "mt = MosesTokenizer(lang='de')\n",
        "print(mt.tokenize(\"Das ist großartig!\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hbz5yrXAsWaF",
      "metadata": {
        "id": "hbz5yrXAsWaF"
      },
      "source": [
        "## NLTK tokenizers\n",
        "\n",
        "Sentence and word tokenization using NLTK (no ML training). Requires punkt models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FKZtvZ77sWaG",
      "metadata": {
        "id": "FKZtvZ77sWaG"
      },
      "source": [
        "### Sentence tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "AJCT0EQtsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJCT0EQtsWaG",
        "outputId": "c6de1b0f-2707-4e7d-fc54-f8689daa9f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello world.', 'How are you?', \"I'm fine.\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Hello world. How are you? I'm fine.\"\n",
        "print(sent_tokenize(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10i_h-BcsWaG",
      "metadata": {
        "id": "10i_h-BcsWaG"
      },
      "source": [
        "### Word tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "qcyfYZSwsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcyfYZSwsWaG",
        "outputId": "1fb2d058-c4aa-4ded-ca85-7f9dc55010b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'split', '!', 'Please', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(\"Don't split! Please.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uxtx18AUsWaG",
      "metadata": {
        "id": "uxtx18AUsWaG"
      },
      "source": [
        "### TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "zyFjlSUPsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyFjlSUPsWaG",
        "outputId": "330e0f8f-980f-432a-f840-8e137314201c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', \"''\", 's', '3.14', '(', 'approx', '.', ')']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "print(TreebankWordTokenizer().tokenize(\"It''s 3.14 (approx.)\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vhnVyb_tsWaG",
      "metadata": {
        "id": "vhnVyb_tsWaG"
      },
      "source": [
        "### RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "dWOJG3hSsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWOJG3hSsWaG",
        "outputId": "e280514d-efb0-42e4-fb6f-6e558b19e3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'world']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tok = RegexpTokenizer(r\"[A-Za-z]+\")\n",
        "print(tok.tokenize(\"Hello, world! 123\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l_yXC4GXsWaG",
      "metadata": {
        "id": "l_yXC4GXsWaG"
      },
      "source": [
        "### Punkt (language-specific)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ugVKf3v5sWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugVKf3v5sWaG",
        "outputId": "b6f8bdef-d762-4114-a0eb-dd13609ee8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dr.', 'Smith went to the U.K.', 'It was fun.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "text = \"Dr. Smith went to the U.K. It was fun.\"\n",
        "print(PunktSentenceTokenizer().tokenize(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caAyjQCPsWaG",
      "metadata": {
        "id": "caAyjQCPsWaG"
      },
      "source": [
        "## spaCy tokenizer (no model)\n",
        "\n",
        "Use spaCy’s rule-based tokenizer by creating a blank Language object."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TiPKP81ZsWaG",
      "metadata": {
        "id": "TiPKP81ZsWaG"
      },
      "source": [
        "### Basic English tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "U46Sut6osWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U46Sut6osWaG",
        "outputId": "87fa1077-698d-4ad0-d3e7-5eba1bc729e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'It', \"'s\", '2025', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "doc = nlp(\"Hello, world! It's 2025.\")\n",
        "print([t.text for t in doc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nov96XnYsWaG",
      "metadata": {
        "id": "nov96XnYsWaG"
      },
      "source": [
        "### Customize token rules (add special case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "Af8NO8ajsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af8NO8ajsWaG",
        "outputId": "693862a7-75e2-4163-8254-d30998a7a11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'NLP', 'and', 'AI', '.']\n"
          ]
        }
      ],
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.symbols import ORTH\n",
        "nlp = English()\n",
        "special_case = [{ORTH: \"NLP\"}]\n",
        "nlp.tokenizer.add_special_case(\"NLP\", special_case)\n",
        "print([t.text for t in nlp(\"I love NLP and AI.\")])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gyND5roLsWaG",
      "metadata": {
        "id": "gyND5roLsWaG"
      },
      "source": [
        "### Split on URLs as single tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "WYVWv5WtsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYVWv5WtsWaG",
        "outputId": "1eb18767-54ab-4fd7-8603-3a49b9ed8720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Read', 'https://example.com', 'now', '.']\n",
            "Install spacy to run.\n"
          ]
        }
      ],
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "doc = nlp(\"Read https://example.com now.\")\n",
        "print([t.text for t in doc])\n",
        "print(\"Install spacy to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Js-ivj-EsWaG",
      "metadata": {
        "id": "Js-ivj-EsWaG"
      },
      "source": [
        "### Whitespace tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "zqT9i-wmsWaG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqT9i-wmsWaG",
        "outputId": "e6b89573-2094-4d49-f59b-8fd136f097ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '  ', 'world']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "nlp.tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab, rules={})  # simplistic\n",
        "print([t.text for t in nlp(\"Hello   world\")])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L3snX2nZsWaG",
      "metadata": {
        "id": "L3snX2nZsWaG"
      },
      "source": [
        "### Measure token attributes (is_punct, is_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "Wt7fbO9tsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt7fbO9tsWaH",
        "outputId": "9cf2f949-8045-4d66-bd92-1419482a0c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hi', False, False), ('!', True, False), (' ', False, True)]\n"
          ]
        }
      ],
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "doc = nlp(\"Hi!  \")\n",
        "print([(t.text, t.is_punct, t.is_space) for t in doc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yRH2oy0tsWaH",
      "metadata": {
        "id": "yRH2oy0tsWaH"
      },
      "source": [
        "## NLTK stemming\n",
        "\n",
        "Porter, Snowball, and Lancaster stemmers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hD9wBEIIsWaH",
      "metadata": {
        "id": "hD9wBEIIsWaH"
      },
      "source": [
        "### PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "uSsDOKpOsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSsDOKpOsWaH",
        "outputId": "599402e7-eb6b-410c-82c4-cf027e4e562e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caress', 'poni', 'caress', 'cat']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print([ps.stem(w) for w in [\"caresses\", \"ponies\", \"caress\", \"cats\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dh6L_GWlsWaH",
      "metadata": {
        "id": "Dh6L_GWlsWaH"
      },
      "source": [
        "### SnowballStemmer (English)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "EA5Rw4UNsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA5Rw4UNsWaH",
        "outputId": "acafd047-75ad-4b8f-c05c-4e75a85fcc4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'easili', 'fair']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print([ss.stem(w) for w in [\"running\", \"easily\", \"fairly\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0HNfztNIsWaH",
      "metadata": {
        "id": "0HNfztNIsWaH"
      },
      "source": [
        "### LancasterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "AjxhDUXAsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjxhDUXAsWaH",
        "outputId": "89619444-ac04-40d3-ae84-bc56e674e214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['maxim', 'presum']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "print([ls.stem(w) for w in [\"maximum\", \"presumably\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5RpbjWi2sWaH",
      "metadata": {
        "id": "5RpbjWi2sWaH"
      },
      "source": [
        "### Compare stemmers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "d9f7bgZEsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9f7bgZEsWaH",
        "outputId": "e00ebff3-ef0e-4f1b-b7dc-e68ff5673962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "univers univers\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "ps, ls = PorterStemmer(), LancasterStemmer()\n",
        "word = \"universities\"\n",
        "print(ps.stem(word), ls.stem(word))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cM7_SP8sWaH",
      "metadata": {
        "id": "1cM7_SP8sWaH"
      },
      "source": [
        "### Stem a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "KIZ69vJnsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIZ69vJnsWaH",
        "outputId": "4948303d-8696-4bfd-9ab0-f5bd48795a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat are run easily.\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "sent = \"Cats are running easily.\"\n",
        "print(\" \".join(ps.stem(w) for w in sent.lower().split()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MbUUwFBSsWaH",
      "metadata": {
        "id": "MbUUwFBSsWaH"
      },
      "source": [
        "## snowballstemmer (standalone)\n",
        "\n",
        "Pure Snowball stemmer package with many languages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25FRPRgXsWaH",
      "metadata": {
        "id": "25FRPRgXsWaH"
      },
      "source": [
        "### English stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "W9pS8Q1zsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9pS8Q1zsWaH",
        "outputId": "0597204b-2a12-4341-ebe7-dff79bd3e097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'happi', 'cat']\n"
          ]
        }
      ],
      "source": [
        "import snowballstemmer\n",
        "stemmer = snowballstemmer.stemmer('english')\n",
        "print(stemmer.stemWords([\"running\", \"happiness\", \"cats\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nTAioMadsWaH",
      "metadata": {
        "id": "nTAioMadsWaH"
      },
      "source": [
        "### Spanish stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "4gjP64aFsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gjP64aFsWaH",
        "outputId": "af8b2054-dc61-44ed-a1ef-c82e5c09a288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['corr', 'felic']\n"
          ]
        }
      ],
      "source": [
        "import snowballstemmer\n",
        "stemmer = snowballstemmer.stemmer('spanish')\n",
        "print(stemmer.stemWords([\"corriendo\", \"felicidad\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ki_GwA1BsWaH",
      "metadata": {
        "id": "ki_GwA1BsWaH"
      },
      "source": [
        "### German stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "S3RbD6UmsWaH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3RbD6UmsWaH",
        "outputId": "39575fb5-a3d7-48a0-ca7f-72f904b0e6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['grossart', 'Kind']\n"
          ]
        }
      ],
      "source": [
        "import snowballstemmer\n",
        "stemmer = snowballstemmer.stemmer('german')\n",
        "print(stemmer.stemWords([\"großartig\", \"Kinder\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GsXenZ6SsWaI",
      "metadata": {
        "id": "GsXenZ6SsWaI"
      },
      "source": [
        "### French stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "U2gKIVwVsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2gKIVwVsWaI",
        "outputId": "2f909c06-289e-4be6-9214-46c31423153d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mang', 'mang']\n"
          ]
        }
      ],
      "source": [
        "import snowballstemmer\n",
        "stemmer = snowballstemmer.stemmer('french')\n",
        "print(stemmer.stemWords([\"manges\", \"mangé\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fN78UIosWaI",
      "metadata": {
        "id": "4fN78UIosWaI"
      },
      "source": [
        "### List supported languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "r95DYQkZsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r95DYQkZsWaI",
        "outputId": "f229bd00-fad0-4ee7-f860-0eab2653ce41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['arabic', 'armenian', 'basque', 'catalan', 'danish', 'dutch', 'dutch_porter', 'english', 'esperanto', 'estonian', 'finnish', 'french', 'german', 'greek', 'hindi', 'hungarian', 'indonesian', 'irish', 'italian', 'lithuanian', 'nepali', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'serbian', 'spanish', 'swedish', 'tamil', 'turkish', 'yiddish']\n"
          ]
        }
      ],
      "source": [
        "import snowballstemmer\n",
        "print(snowballstemmer.algorithms())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3qBRrGSsWaI",
      "metadata": {
        "id": "Z3qBRrGSsWaI"
      },
      "source": [
        "## simplemma\n",
        "\n",
        "Dictionary-based lemmatizer for many languages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vwPFgme7sWaI",
      "metadata": {
        "id": "vwPFgme7sWaI"
      },
      "source": [
        "### English lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "y5rRVkLpsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5rRVkLpsWaI",
        "outputId": "9c8e77ed-67eb-4dee-a89c-0aee8cd564f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'mouse', 'good']\n"
          ]
        }
      ],
      "source": [
        "from simplemma import lemmatize\n",
        "\n",
        "print([lemmatize(w, lang=\"en\") for w in [\"running\", \"mice\", \"better\"]])\n",
        "# e.g. ['run', 'mouse', 'better']  (irregular adjectives may stay as-is)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WChOAfyAsWaI",
      "metadata": {
        "id": "WChOAfyAsWaI"
      },
      "source": [
        "### French lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "AfacGNQfsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfacGNQfsWaI",
        "outputId": "69a9a223-a52c-4d02-a6a1-457976b39e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['manger', 'allée']\n"
          ]
        }
      ],
      "source": [
        "from simplemma import lemmatize\n",
        "from simplemma import lemmatize\n",
        "\n",
        "words = [\"manges\", \"allées\"]\n",
        "print([lemmatize(w, lang=\"fr\") for w in words])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YlI8DwtFsWaI",
      "metadata": {
        "id": "YlI8DwtFsWaI"
      },
      "source": [
        "### German lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "Q9n2FvjmsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9n2FvjmsWaI",
        "outputId": "c53f59f8-d457-4d25-b4af-61858a73d408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Kind', 'groß']\n"
          ]
        }
      ],
      "source": [
        "from simplemma import lemmatize\n",
        "\n",
        "words = [\"Kinder\", \"großen\"]\n",
        "print([lemmatize(w, lang=\"de\") for w in words])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Cv2rQjYsWaI",
      "metadata": {
        "id": "_Cv2rQjYsWaI"
      },
      "source": [
        "### Mixed-language handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "cje9tYV3sWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cje9tYV3sWaI",
        "outputId": "9ca67e78-6cae-48ce-9228-0fef633e12a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'manges']\n",
            "['running', 'manger']\n"
          ]
        }
      ],
      "source": [
        "import simplemma\n",
        "from simplemma import lemmatize\n",
        "\n",
        "\n",
        "words = [\"running\", \"manges\"]\n",
        "print([lemmatize(w, \"en\") for w in words])  # e.g. ['run', 'manges']\n",
        "print([lemmatize(w, \"fr\") for w in words])  # e.g. ['running', 'manger']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8yLFrBvsWaI",
      "metadata": {
        "id": "V8yLFrBvsWaI"
      },
      "source": [
        "### Sentence lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "p42V7xCysWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p42V7xCysWaI",
        "outputId": "b5ba3644-600e-40a2-a441-53fa856dfec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat be run fast\n"
          ]
        }
      ],
      "source": [
        "from simplemma import lemmatize\n",
        "\n",
        "sent = \"Cats were running faster\"\n",
        "print(\" \".join(lemmatize(w, \"en\") for w in sent.split()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rFkP32MVsWaI",
      "metadata": {
        "id": "rFkP32MVsWaI"
      },
      "source": [
        "## flashtext\n",
        "\n",
        "Very fast exact keyword extraction and replacement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U4jZ_uMMsWaI",
      "metadata": {
        "id": "U4jZ_uMMsWaI"
      },
      "source": [
        "### Extract keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "Fva8isVhsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fva8isVhsWaI",
        "outputId": "cf7c8187-cd5a-4bf0-b5f5-d1d66e116304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['python', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "kp = KeywordProcessor(case_sensitive=False)\n",
        "kp.add_keywords_from_list([\"nlp\", \"genai\", \"python\"])\n",
        "text = \"I love Python and NLP!\"\n",
        "print(kp.extract_keywords(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NGxVkz6wsWaI",
      "metadata": {
        "id": "NGxVkz6wsWaI"
      },
      "source": [
        "### Add mapping (synonym → canonical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "bITLzZuTsWaI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bITLzZuTsWaI",
        "outputId": "409f5ebe-12dc-4c94-d213-e6bd7f1e736d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial_intelligence']\n"
          ]
        }
      ],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "kp = KeywordProcessor(case_sensitive=False)\n",
        "kp.add_keyword(\"ai\", \"artificial_intelligence\")\n",
        "print(kp.extract_keywords(\"AI rocks\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lM-YJRXYsWaJ",
      "metadata": {
        "id": "lM-YJRXYsWaJ"
      },
      "source": [
        "### Replace keywords inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "2wGU0aWFsWaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wGU0aWFsWaJ",
        "outputId": "b818f971-8922-4183-e713-516545883062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "International Business Machines Cloud\n"
          ]
        }
      ],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "kp = KeywordProcessor()\n",
        "kp.add_keyword(\"IBM\", \"International Business Machines\")\n",
        "print(kp.replace_keywords(\"IBM Cloud\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BjaAFflTsWaJ",
      "metadata": {
        "id": "BjaAFflTsWaJ"
      },
      "source": [
        "### Large dictionaries (performance hint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "jkdOO2DrsWaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkdOO2DrsWaJ",
        "outputId": "7bc5d8ca-78d2-4e81-9ec9-cca6be8ecb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lang', 'lang']\n"
          ]
        }
      ],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "kp = KeywordProcessor()\n",
        "kp.add_keywords_from_dict({\"lang\": [\"Python\", \"Java\", \"C++\"]})\n",
        "print(kp.extract_keywords(\"I code in Python and C++\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s5a_zXB1sWaJ",
      "metadata": {
        "id": "s5a_zXB1sWaJ"
      },
      "source": [
        "### Keyword spans (offsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PVHeOynlsWaJ",
      "metadata": {
        "id": "PVHeOynlsWaJ"
      },
      "source": [
        "## pyahocorasick\n",
        "\n",
        "Aho–Corasick automaton for multi-pattern substring search."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RhwGmU2zsWaJ",
      "metadata": {
        "id": "RhwGmU2zsWaJ"
      },
      "source": [
        "### Build automaton and search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "7SKZkkwHsWaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SKZkkwHsWaJ",
        "outputId": "1953a30a-e508-40b9-9578-fc4ca097657d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 python\n"
          ]
        }
      ],
      "source": [
        "import ahocorasick\n",
        "A = ahocorasick.Automaton()\n",
        "for i, w in enumerate([\"nlp\", \"python\", \"ai\"]):\n",
        "    A.add_word(w, (i, w))\n",
        "A.make_automaton()\n",
        "for end_idx, (i, w) in A.iter(\"I love python and NLP\"):\n",
        "    print(end_idx, w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SgDlh4v4sWaJ",
      "metadata": {
        "id": "SgDlh4v4sWaJ"
      },
      "source": [
        "### Find first match only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "a9lawDg4sWaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9lawDg4sWaJ",
        "outputId": "279ef5d8-2e30-4f5a-8398-49c5fea688f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 'bar')\n"
          ]
        }
      ],
      "source": [
        "import ahocorasick\n",
        "A = ahocorasick.Automaton()\n",
        "for w in [\"foo\", \"bar\"]:\n",
        "    A.add_word(w, w)\n",
        "A.make_automaton()\n",
        "print(next(A.iter(\"barbecue\")))  # first match\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9gdohuq_sWaJ",
      "metadata": {
        "id": "9gdohuq_sWaJ"
      },
      "source": [
        "### Store custom payloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "zD5eEmKhsWaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD5eEmKhsWaJ",
        "outputId": "093cd3b4-3214-42d4-d6ff-30ac4b7c1e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2, {'tag': 'tech'})]\n",
            "Install pyahocorasick to run.\n"
          ]
        }
      ],
      "source": [
        "import ahocorasick\n",
        "A = ahocorasick.Automaton()\n",
        "A.add_word(\"nlp\", {\"tag\":\"tech\"})\n",
        "A.make_automaton()\n",
        "print(list(A.iter(\"nlp rules\")))\n",
        "print(\"Install pyahocorasick to run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SlmFfzQksWaJ",
      "metadata": {
        "id": "SlmFfzQksWaJ"
      },
      "source": [
        "### Case-insensitive build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "IEBV04PqsWaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEBV04PqsWaJ",
        "outputId": "bc0d4408-566c-4be7-c612-840bbd84f9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python']\n"
          ]
        }
      ],
      "source": [
        "import ahocorasick\n",
        "A = ahocorasick.Automaton()\n",
        "for w in [\"NLP\",\"Python\"]:\n",
        "    A.add_word(w.lower(), w)\n",
        "A.make_automaton()\n",
        "text = \"I like PYTHON\"\n",
        "print([v for _, v in A.iter(text.lower())])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ruf5UcP1sWaJ",
      "metadata": {
        "id": "Ruf5UcP1sWaJ"
      },
      "source": [
        "## rapidfuzz\n",
        "\n",
        "Fast fuzzy matching (ratios, extract, distances)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JKrNk7Z4sWaJ",
      "metadata": {
        "id": "JKrNk7Z4sWaJ"
      },
      "source": [
        "### Similarity ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "U4k8_FTOsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4k8_FTOsWaK",
        "outputId": "732dc579-1176-466d-a3c6-d506fb81f734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.9090909090909\n"
          ]
        }
      ],
      "source": [
        "from rapidfuzz import fuzz\n",
        "print(fuzz.ratio(\"house\", \"houses\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qA0GhSX_sWaK",
      "metadata": {
        "id": "qA0GhSX_sWaK"
      },
      "source": [
        "### Partial ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "MTwJ1RVdsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTwJ1RVdsWaK",
        "outputId": "99f965ec-61fd-48d3-b2c5-d19c326fbe56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0\n"
          ]
        }
      ],
      "source": [
        "from rapidfuzz import fuzz\n",
        "print(fuzz.partial_ratio(\"New York City\", \"York\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vtxaWcqcsWaK",
      "metadata": {
        "id": "vtxaWcqcsWaK"
      },
      "source": [
        "### Token sort ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "q_nlUgxKsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_nlUgxKsWaK",
        "outputId": "20f18f4c-754b-4c24-d64f-83ccde6404f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66.66666666666667\n"
          ]
        }
      ],
      "source": [
        "from rapidfuzz import fuzz\n",
        "print(fuzz.token_sort_ratio(\"NLP and AI\", \"AI & NLP\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OZkAX_HdsWaK",
      "metadata": {
        "id": "OZkAX_HdsWaK"
      },
      "source": [
        "### Top matches from choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "7QbXwwxCsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QbXwwxCsWaK",
        "outputId": "6ba67fb3-4031-4d8d-a819-b5d604c0c858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('python', 90.0, 0), ('javascript', 45.0, 2)]\n"
          ]
        }
      ],
      "source": [
        "from rapidfuzz import process\n",
        "choices = [\"python\", \"java\", \"javascript\", \"rust\"]\n",
        "print(process.extract(\"py\", choices, limit=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7PkCdN7FsWaK",
      "metadata": {
        "id": "7PkCdN7FsWaK"
      },
      "source": [
        "### Levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "5d6h-XtnsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d6h-XtnsWaK",
        "outputId": "a664963d-8a2d-4a11-c551-623e08f50cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "from rapidfuzz.distance import Levenshtein\n",
        "print(Levenshtein.distance(\"kitten\", \"sitting\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X1EAJAkusWaK",
      "metadata": {
        "id": "X1EAJAkusWaK"
      },
      "source": [
        "## regex (enhanced regex module)\n",
        "\n",
        "Advanced features: overlapped matches, fuzzy matching, named sets, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "auLnrVe-sWaK",
      "metadata": {
        "id": "auLnrVe-sWaK"
      },
      "source": [
        "### Overlapped matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "0LlIdmfnsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LlIdmfnsWaK",
        "outputId": "7a87679a-ee79-49a6-f30f-2f1fd18465cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aba', 'aba']\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "print(re.findall(r\"(?r)aba\", \"ababa\", overlapped=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "STkfo6mzsWaK",
      "metadata": {
        "id": "STkfo6mzsWaK"
      },
      "source": [
        "### Fuzzy matching (up to 1 error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "RapKWnqSsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RapKWnqSsWaK",
        "outputId": "16f527ab-2690-4e68-c22c-bfaa40f43226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "m = re.search(r\"(?:color){e<=1}\", \"colour\")\n",
        "print(bool(m))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v-HMxzp_sWaK",
      "metadata": {
        "id": "v-HMxzp_sWaK"
      },
      "source": [
        "### Named sets / character properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "rZi9gp1HsWaK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZi9gp1HsWaK",
        "outputId": "f46cb2c7-1d34-48b1-a7c3-083af08ad6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "print(bool(re.match(r\"\\p{L}+\", \"Café\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODLYdJqDsWaL",
      "metadata": {
        "id": "ODLYdJqDsWaL"
      },
      "source": [
        "## spaCy Matcher & PhraseMatcher (rule‑based)\n",
        "\n",
        "Build token patterns and phrase matchers without ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jdRHWXe3sWaL",
      "metadata": {
        "id": "jdRHWXe3sWaL"
      },
      "source": [
        "### Matcher: adjective + noun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "TBFzPmYLsWaL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBFzPmYLsWaL",
        "outputId": "62a7d326-3393-400f-fdfe-1f1076d8ab82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great work', 'awesome work']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "nlp = English()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]  # POS needs model; use TEXT for no-model demo\n",
        "#No model available → use TEXT rules instead:\n",
        "pattern = [{\"LOWER\": {\"IN\": [\"great\",\"awesome\",\"good\"]}}, {\"LOWER\": \"work\"}]\n",
        "matcher.add(\"ADJ_NOUN\", [pattern])\n",
        "doc = nlp(\"great work by team; awesome work indeed\")\n",
        "print([doc[s:e].text for _, s, e in matcher(doc)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlQUe9SzsWaL",
      "metadata": {
        "id": "VlQUe9SzsWaL"
      },
      "source": [
        "### PhraseMatcher: canned phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wKHUCSnSsWaL",
      "metadata": {
        "id": "wKHUCSnSsWaL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "nlp = English()\n",
        "ph = PhraseMatcher(nlp.vocab)\n",
        "phrases = [nlp.make_doc(p) for p in [\"natural language processing\",\"machine translation\"]]\n",
        "ph.add(\"TECH\", phrases)\n",
        "doc = nlp(\"I study natural language processing.\")\n",
        "print([doc[s:e].text for _, s, e in ph(doc)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q_oGEKU3sWaL",
      "metadata": {
        "id": "q_oGEKU3sWaL"
      },
      "source": [
        "### Get spans with offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wlpGjShWsWaL",
      "metadata": {
        "id": "wlpGjShWsWaL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "nlp = English()\n",
        "ph = PhraseMatcher(nlp.vocab)\n",
        "ph.add(\"BRANDS\", [nlp.make_doc(\"IBM\"), nlp.make_doc(\"OpenAI\")])\n",
        "doc = nlp(\"IBM partners with OpenAI.\")\n",
        "print([(doc[s:e].text, s, e) for _, s, e in ph(doc)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O4ng-gaEsWaL",
      "metadata": {
        "id": "O4ng-gaEsWaL"
      },
      "source": [
        "### Chaining multiple patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9HBfbjIMsWaL",
      "metadata": {
        "id": "9HBfbjIMsWaL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "nlp = English()\n",
        "m = Matcher(nlp.vocab)\n",
        "m.add(\"EMAIL\", [[{\"TEXT\": {\"REGEX\": \".+@.+\"}}]])\n",
        "m.add(\"HASHTAG\", [[{\"TEXT\": {\"REGEX\": r\"#\\w+\"}}]])\n",
        "doc = nlp(\"Mail a@b.com #hashtag\")\n",
        "print([doc[s:e].text for _, s, e in m(doc)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TQG6oz6ZsWaL",
      "metadata": {
        "id": "TQG6oz6ZsWaL"
      },
      "source": [
        "### On-the-fly token attributes (LIKE_NUM/IS_PUNCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LVkOOSMQsWaL",
      "metadata": {
        "id": "LVkOOSMQsWaL"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "doc = nlp(\"I have 2 apples, you have 3.\")\n",
        "print([(t.text, t.like_num, t.is_punct) for t in doc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nz0Be8ZMsWaL",
      "metadata": {
        "id": "Nz0Be8ZMsWaL"
      },
      "source": [
        "## dateparser\n",
        "\n",
        "Parse natural language dates and extract all date mentions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pl1pUeWYsWaL",
      "metadata": {
        "id": "pl1pUeWYsWaL"
      },
      "source": [
        "### Parse simple dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kLFuBlCzsWaL",
      "metadata": {
        "id": "kLFuBlCzsWaL"
      },
      "outputs": [],
      "source": [
        "import dateparser\n",
        "print(dateparser.parse(\"15 Sep 2025\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ktirXwJtsWaL",
      "metadata": {
        "id": "ktirXwJtsWaL"
      },
      "source": [
        "### Relative dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FEcF2MS3sWaL",
      "metadata": {
        "id": "FEcF2MS3sWaL"
      },
      "outputs": [],
      "source": [
        "import dateparser\n",
        "print(dateparser.parse(\"next Friday at 5pm\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AFUbW1TmsWaL",
      "metadata": {
        "id": "AFUbW1TmsWaL"
      },
      "source": [
        "### Locale-specific formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCtZc4q_sWaL",
      "metadata": {
        "id": "bCtZc4q_sWaL"
      },
      "outputs": [],
      "source": [
        "import dateparser\n",
        "print(dateparser.parse(\"15/09/2025\", settings={\"DATE_ORDER\":\"DMY\"}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZZzU6meFsWaM",
      "metadata": {
        "id": "ZZzU6meFsWaM"
      },
      "source": [
        "### Search and extract all dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QdXXRt34sWaM",
      "metadata": {
        "id": "QdXXRt34sWaM"
      },
      "outputs": [],
      "source": [
        "from dateparser.search import search_dates\n",
        "print(search_dates(\"Pay on 15 Sep or 20/09/2025\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uglWtlmesWaM",
      "metadata": {
        "id": "uglWtlmesWaM"
      },
      "source": [
        "### Time zone handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6YDL8tsWaM",
      "metadata": {
        "id": "7f6YDL8tsWaM"
      },
      "outputs": [],
      "source": [
        "import dateparser\n",
        "print(dateparser.parse(\"2025-09-15 10:00 IST\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o_yPWbKHsWaM",
      "metadata": {
        "id": "o_yPWbKHsWaM"
      },
      "source": [
        "## wordfreq\n",
        "\n",
        "Word frequency estimates and helpful lists."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mAErsx58sWaM",
      "metadata": {
        "id": "mAErsx58sWaM"
      },
      "source": [
        "### Zipf frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ER_JSvNvsWaM",
      "metadata": {
        "id": "ER_JSvNvsWaM"
      },
      "outputs": [],
      "source": [
        "from wordfreq import zipf_frequency\n",
        "print(zipf_frequency(\"python\", \"en\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FARQMr40sWaM",
      "metadata": {
        "id": "FARQMr40sWaM"
      },
      "source": [
        "### Word frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LotynATrsWaM",
      "metadata": {
        "id": "LotynATrsWaM"
      },
      "outputs": [],
      "source": [
        "from wordfreq import word_frequency\n",
        "print(word_frequency(\"hyderabad\", \"en\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFIUc91IsWaM",
      "metadata": {
        "id": "oFIUc91IsWaM"
      },
      "source": [
        "### Top N words by language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XI0v5vBksWaM",
      "metadata": {
        "id": "XI0v5vBksWaM"
      },
      "outputs": [],
      "source": [
        "from wordfreq import top_n_list\n",
        "print(top_n_list(\"en\", n=10)[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hb36AXmRsWaM",
      "metadata": {
        "id": "Hb36AXmRsWaM"
      },
      "source": [
        "### Filter rare tokens (threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7ux-ZmsWaM",
      "metadata": {
        "id": "4e7ux-ZmsWaM"
      },
      "outputs": [],
      "source": [
        "from wordfreq import zipf_frequency\n",
        "words = \"this is a rarewordxyz maybe\".split()\n",
        "print([w for w in words if zipf_frequency(w,\"en\") > 2.0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DITuvCHRsWaM",
      "metadata": {
        "id": "DITuvCHRsWaM"
      },
      "source": [
        "### Compare frequencies across words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jnLbjU6EsWaM",
      "metadata": {
        "id": "jnLbjU6EsWaM"
      },
      "outputs": [],
      "source": [
        "from wordfreq import zipf_frequency\n",
        "items = [\"python\",\"java\",\"elixir\"]\n",
        "print(sorted(items, key=lambda w: zipf_frequency(w,\"en\"), reverse=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mUfwAstvsWaM",
      "metadata": {
        "id": "mUfwAstvsWaM"
      },
      "source": [
        "## textstat\n",
        "\n",
        "Readability scores and text statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OKa4z0vYsWaM",
      "metadata": {
        "id": "OKa4z0vYsWaM"
      },
      "source": [
        "### Flesch Reading Ease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k-vwAdJIsWaM",
      "metadata": {
        "id": "k-vwAdJIsWaM"
      },
      "outputs": [],
      "source": [
        "import textstat\n",
        "print(textstat.flesch_reading_ease(\"This is a simple sentence.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wFluNyFJsWaM",
      "metadata": {
        "id": "wFluNyFJsWaM"
      },
      "source": [
        "### Grade level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hAQ7F-s9sWaM",
      "metadata": {
        "id": "hAQ7F-s9sWaM"
      },
      "outputs": [],
      "source": [
        "import textstat\n",
        "print(textstat.text_standard(\"Complex, polysyllabic content may score differently.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UUyXH85jsWaM",
      "metadata": {
        "id": "UUyXH85jsWaM"
      },
      "source": [
        "### Syllable count & char count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wjh3mXb8sWaN",
      "metadata": {
        "id": "Wjh3mXb8sWaN"
      },
      "outputs": [],
      "source": [
        "import textstat\n",
        "s = \"Readability matters.\"\n",
        "print(textstat.syllable_count(s), textstat.char_count(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WL42SyqJsWaN",
      "metadata": {
        "id": "WL42SyqJsWaN"
      },
      "source": [
        "### Sentence count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T84IvQrIsWaN",
      "metadata": {
        "id": "T84IvQrIsWaN"
      },
      "outputs": [],
      "source": [
        "import textstat\n",
        "print(textstat.sentence_count(\"One. Two! Three?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FQL4hcjxsWaN",
      "metadata": {
        "id": "FQL4hcjxsWaN"
      },
      "source": [
        "## pyenchant\n",
        "\n",
        "Dictionary-based spell checking and suggestions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16k30KOLsWaN",
      "metadata": {
        "id": "16k30KOLsWaN"
      },
      "source": [
        "### Check spelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S2fIY3KOsWaN",
      "metadata": {
        "id": "S2fIY3KOsWaN"
      },
      "outputs": [],
      "source": [
        "import enchant\n",
        "d = enchant.Dict(\"en_US\")\n",
        "print(d.check(\"color\"), d.check(\"colur\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WER-usiIsWaN",
      "metadata": {
        "id": "WER-usiIsWaN"
      },
      "source": [
        "## symspellpy\n",
        "\n",
        "Very fast spell correction using SymSpell algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x5X6buKhsWaN",
      "metadata": {
        "id": "x5X6buKhsWaN"
      },
      "source": [
        "### Create dictionary and add words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gd9wjhCTsWaO",
      "metadata": {
        "id": "gd9wjhCTsWaO"
      },
      "outputs": [],
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "sym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "for w,f in [(\"color\",10),(\"colour\",5),(\"colors\",3)]:\n",
        "    sym.create_dictionary_entry(w, f)\n",
        "print(sym.lookup(\"colur\", Verbosity.CLOSEST))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wXaLIWp0sWaO",
      "metadata": {
        "id": "wXaLIWp0sWaO"
      },
      "source": [
        "### Compound word correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fFJIgfrsWaO",
      "metadata": {
        "id": "3fFJIgfrsWaO"
      },
      "source": [
        "## vaderSentiment (lexicon-based sentiment)\n",
        "\n",
        "Social-text‑oriented sentiment analyzer; no ML required."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dyJAi4KsWaO",
      "metadata": {
        "id": "5dyJAi4KsWaO"
      },
      "source": [
        "### Basic polarity scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vgLeUg61sWaO",
      "metadata": {
        "id": "vgLeUg61sWaO"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "print(sid.polarity_scores(\"VADER is extremely useful!\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Wows0JCsWaO",
      "metadata": {
        "id": "4Wows0JCsWaO"
      },
      "source": [
        "### Negation handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VfUdS_FSsWaO",
      "metadata": {
        "id": "VfUdS_FSsWaO"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "print(sid.polarity_scores(\"I don't like this.\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RudG-jtEsWaO",
      "metadata": {
        "id": "RudG-jtEsWaO"
      },
      "source": [
        "### Emoji & slang handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FGPP2tA3sWaO",
      "metadata": {
        "id": "FGPP2tA3sWaO"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "print(sid.polarity_scores(\"This is lit 🔥\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fcr5O4F8sWaO",
      "metadata": {
        "id": "Fcr5O4F8sWaO"
      },
      "source": [
        "### Contrastive conjunctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s-lKnsJ2sWaO",
      "metadata": {
        "id": "s-lKnsJ2sWaO"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "print(sid.polarity_scores(\"The plot was boring, but the acting was great!\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0zPWHWu7sWaO",
      "metadata": {
        "id": "0zPWHWu7sWaO"
      },
      "source": [
        "### Batch evaluate lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UrzHWjZ0sWaP",
      "metadata": {
        "id": "UrzHWjZ0sWaP"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "lines = [\"good\", \"bad\", \"meh\"]\n",
        "print([sid.polarity_scores(s)[\"compound\"] for s in lines])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E9Sz1uNasWaP",
      "metadata": {
        "id": "E9Sz1uNasWaP"
      },
      "source": [
        "## textblob (Pattern-based)\n",
        "\n",
        "Convenient API for sentiment (lexicon-based), noun phrases, simple POS, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K34GcC6ysWaP",
      "metadata": {
        "id": "K34GcC6ysWaP"
      },
      "source": [
        "### Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e097bQQsWaP",
      "metadata": {
        "id": "4e097bQQsWaP"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "print(TextBlob(\"I absolutely love this!\").sentiment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QiT3l_NrsWaP",
      "metadata": {
        "id": "QiT3l_NrsWaP"
      },
      "source": [
        "### Tokens & noun phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rfY6vg1zsWaP",
      "metadata": {
        "id": "rfY6vg1zsWaP"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "print(TextBlob(\"I havv goood speling\").correct())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p0QJ-6tEsWaP",
      "metadata": {
        "id": "p0QJ-6tEsWaP"
      },
      "source": [
        "## indic-transliteration\n",
        "\n",
        "Script ↔ script transliteration (e.g., Devanagari ⇄ IAST/ISO)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FAjTkHh_sWaP",
      "metadata": {
        "id": "FAjTkHh_sWaP"
      },
      "source": [
        "### Devanagari → IAST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ePgUj8oFsWaP",
      "metadata": {
        "id": "ePgUj8oFsWaP"
      },
      "outputs": [],
      "source": [
        "from indic_transliteration.sanscript import transliterate, DEVANAGARI, IAST\n",
        "print(transliterate(\"भारत\", DEVANAGARI, IAST))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C4-ZrIqisWaP",
      "metadata": {
        "id": "C4-ZrIqisWaP"
      },
      "source": [
        "### IAST → Devanagari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YYJ2saYhsWaP",
      "metadata": {
        "id": "YYJ2saYhsWaP"
      },
      "outputs": [],
      "source": [
        "from indic_transliteration.sanscript import transliterate, DEVANAGARI, IAST\n",
        "print(transliterate(\"bhārata\", IAST, DEVANAGARI))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "swmcNOj7sWaP",
      "metadata": {
        "id": "swmcNOj7sWaP"
      },
      "source": [
        "### Devanagari → HK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p-JDoMLpsWaP",
      "metadata": {
        "id": "p-JDoMLpsWaP"
      },
      "outputs": [],
      "source": [
        "from indic_transliteration.sanscript import transliterate, DEVANAGARI, HK\n",
        "print(transliterate(\"संस्कृतम्\", DEVANAGARI, HK))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFCKVp-5sWaQ",
      "metadata": {
        "id": "oFCKVp-5sWaQ"
      },
      "source": [
        "### Custom mapping example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FI2Wa1eSsWaQ",
      "metadata": {
        "id": "FI2Wa1eSsWaQ"
      },
      "outputs": [],
      "source": [
        "from indic_transliteration.sanscript import transliterate, DEVANAGARI, ITRANS\n",
        "print(transliterate(\"नमस्ते\", DEVANAGARI, ITRANS))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LqSmXdp6sWaQ",
      "metadata": {
        "id": "LqSmXdp6sWaQ"
      },
      "source": [
        "### Multiple words/batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s0hvkwl9sWaQ",
      "metadata": {
        "id": "s0hvkwl9sWaQ"
      },
      "outputs": [],
      "source": [
        "from indic_transliteration.sanscript import transliterate, DEVANAGARI, IAST\n",
        "items = [\"भारत\", \"सम्मान\"]\n",
        "print([transliterate(x, DEVANAGARI, IAST) for x in items])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nre4RG5QsWaQ",
      "metadata": {
        "id": "nre4RG5QsWaQ"
      },
      "source": [
        "## indic-nlp-library\n",
        "\n",
        "Normalization, tokenization, and utilities for Indic languages. Requires resource path setup."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IMaWnQx3sWaQ",
      "metadata": {
        "id": "IMaWnQx3sWaQ"
      },
      "source": [
        "### Set INDIC_NLP_RESOURCES and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6wycJ2OsWaQ",
      "metadata": {
        "id": "b6wycJ2OsWaQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['INDIC_NLP_RESOURCES'] = \"/path/to/indic_nlp_resources\"  # set your path\n",
        "from indicnlp import common\n",
        "print(\"Resources path:\", os.environ['INDIC_NLP_RESOURCES'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ne8Uebl0sWaQ",
      "metadata": {
        "id": "Ne8Uebl0sWaQ"
      },
      "source": [
        "### Normalize Hindi text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3uH2nGagsWaQ",
      "metadata": {
        "id": "3uH2nGagsWaQ"
      },
      "outputs": [],
      "source": [
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "factory = IndicNormalizerFactory()\n",
        "normalizer = factory.get_normalizer(\"hi\")\n",
        "print(normalizer.normalize(\"क़िलाफ़\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5yE_rUVssWaS",
      "metadata": {
        "id": "5yE_rUVssWaS"
      },
      "source": [
        "### Sentence tokenization (Hindi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zjv59ParsWaS",
      "metadata": {
        "id": "zjv59ParsWaS"
      },
      "source": [
        "## pykakasi\n",
        "\n",
        "Japanese transliteration (Kanji/Kana → Hepburn Romaji, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gjYVAN5xsWaS",
      "metadata": {
        "id": "gjYVAN5xsWaS"
      },
      "source": [
        "### Kanji → Romaji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ezFW1GysWaT",
      "metadata": {
        "id": "0ezFW1GysWaT"
      },
      "outputs": [],
      "source": [
        "import pykakasi\n",
        "kakasi = pykakasi.kakasi()\n",
        "kakasi.setMode(\"H\",\"a\"); kakasi.setMode(\"K\",\"a\"); kakasi.setMode(\"J\",\"a\")\n",
        "conv = kakasi.getConverter()\n",
        "print(conv.do(\"日本語の勉強\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E71SZUfzsWaT",
      "metadata": {
        "id": "E71SZUfzsWaT"
      },
      "source": [
        "### Kana → Romaji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_XQyU7wnsWaT",
      "metadata": {
        "id": "_XQyU7wnsWaT"
      },
      "outputs": [],
      "source": [
        "import pykakasi\n",
        "kakasi = pykakasi.kakasi()\n",
        "conv = kakasi.getConverter()\n",
        "print(conv.do(\"にほんご\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}