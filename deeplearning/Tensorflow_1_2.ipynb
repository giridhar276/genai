{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq0k4z1Z0g6m"
      },
      "source": [
        "![title](img/tensorflow4.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ir4v2EZ0g6o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
        "train_x, test_x = train_x/255.0, test_x/255.0\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2FPvpQi0g6o"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "buffer_size = 10000\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32).shuffle(10000)\n",
        "training_dataset = training_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
        "training_dataset = training_dataset.repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_QFHqK60g6o"
      },
      "outputs": [],
      "source": [
        "testing_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size).shuffle(10000)\n",
        "testing_dataset = training_dataset.repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5lglXCQ0g6o"
      },
      "source": [
        "#### Building the model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQluDzg00g6p"
      },
      "source": [
        "**### Flatten()**\n",
        "\n",
        "Purpose: reshapes inputs (like a 28×28 image) into one long list of numbers (28*28=784).\n",
        "\n",
        "Why: Dense layers expect a 1-D vector, not a 2-D image.\n",
        "\n",
        "Not a hidden layer; it’s just formatting the data.\n",
        "\n",
        "\n",
        "#### Dense(512, activation='relu') ← Hidden layer **bold text**\n",
        "\n",
        "Purpose: learns features/patterns from the input.\n",
        "\n",
        "512 = number of “little calculators” (neurons). More neurons ⇒ more capacity to learn.\n",
        "\n",
        "relu = activation function that helps the model learn non-linear patterns and train faster.\n",
        "\n",
        "**#### Dropout(0.2)**\n",
        "\n",
        "Purpose: during training, randomly turns off 20% of those 512 neurons each step.\n",
        "\n",
        "Why: prevents over-reliance on a few neurons (reduces overfitting).\n",
        "\n",
        "Not a hidden layer; it doesn’t produce new features, it just “thins” the hidden layer while training.\n",
        "\n",
        "**#### Dense(10, activation='softmax') ← Output layer**\n",
        "\n",
        "Purpose: produces 10 probabilities (one per class, e.g., digits 0–9).\n",
        "\n",
        "softmax = converts raw scores to probabilities that add to 1.\n",
        "\n",
        "Not hidden; this is the final decision layer you read as the model’s prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY9BnTta0g6p"
      },
      "outputs": [],
      "source": [
        "#Now in the fit() function, we can pass the dataset directly in, as follows:\n",
        "model5 = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahmU-F4z0g6p"
      },
      "source": [
        "#### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmZydpP_0g6p"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = len(train_x)//batch_size #required becuase of the repeat() on the dataset\n",
        "optimiser = tf.keras.optimizers.Adam()\n",
        "model5.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Nsbz-20g6p"
      },
      "source": [
        "#### Fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5wj91040g6p"
      },
      "outputs": [],
      "source": [
        "model5.fit(training_dataset, epochs=epochs, steps_per_epoch = steps_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMZxImiV0g6q"
      },
      "source": [
        "#### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT92JZ4O0g6q"
      },
      "outputs": [],
      "source": [
        "model5.evaluate(testing_dataset,steps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GH0ibmI0g6q"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "callbacks = [\n",
        "  # Write TensorBoard logs to `./logs` directory\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='log/{}/'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hagil-Bt0g6q"
      },
      "outputs": [],
      "source": [
        "model5.fit(training_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=testing_dataset,\n",
        "          validation_steps=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KO_TJTg0g6q"
      },
      "source": [
        "#### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOhzqlgu0g6q"
      },
      "outputs": [],
      "source": [
        "model5.evaluate(testing_dataset,steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z5mAZoN0g6q"
      },
      "source": [
        "## Saving and loading Keras models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTOeHaF0g6q"
      },
      "source": [
        ">The Keras API in TensorFlow has the ability to save and restore models easily. This is done as follows, and saves the model in the current directory. Of course, a longer path may be passed here:\n",
        "\n",
        "#### Saving a model\n",
        "    \n",
        "`model.save('./model_name.h5')`\n",
        "\n",
        ">This will save the model architecture, its weights, its training state (loss, optimizer), and the state of the optimizer, so that you can carry on training the model from where you left off.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vow2BVNd0g6q"
      },
      "source": [
        ">Loading a saved model is done as follows. Note that if you have compiled your model, the load will compile your model using the saved training configuration:\n",
        "\n",
        "#### Loding a model\n",
        "\n",
        "`from tensorflow.keras.models import load_model\n",
        "new_model = load_model('./model_name.h5')`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvUVwBtK0g6q"
      },
      "source": [
        ">It is also possible to save just the model weights and load them with this (in which case, you must build your architecture to load the weights into):\n",
        "\n",
        "#### Saving the model weights only\n",
        "    \n",
        "    `model.save_weights('./model_weights.h5')`\n",
        "    \n",
        ">Then use the following to load it:\n",
        "\n",
        "#### Loding the weights\n",
        "    \n",
        "    `model.load_weights('./model_weights.h5')`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL-n9vlQ16-8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qgXp0Yz17Fi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJXr6LHw17JZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFoWrF6A17Mk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlLVUf9117Pg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeW8IlqM0g6r"
      },
      "source": [
        "# Keras datasets\n",
        "\n",
        ">The following datasets are available from within Keras: boston_housing, cifar10, cifar100, fashion_mnist, imdb, mnist,and reuters.\n",
        "\n",
        ">They are all accessed with the function.\n",
        "\n",
        "`load_data()`  \n",
        "\n",
        ">For example, to load the fashion_mnist dataset, use the following:\n",
        "\n",
        "`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZ5JhdW0g6r"
      },
      "source": [
        "![title](img/dataset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2CSBOYG0g6r"
      },
      "source": [
        "### Using NumPy arrays with datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55XggTtU0g6r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "number_items = 11\n",
        "number_list1 = np.arange(number_items)\n",
        "number_list2 = np.arange(number_items,number_items*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WETLw-rL0g6r"
      },
      "source": [
        "#### Create datasets, using the from_tensor_slices() method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hNRZ5Uz0g6r"
      },
      "outputs": [],
      "source": [
        "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfae36Ua0g6r"
      },
      "source": [
        "#### Create an iterator on it using the make_one_shot_iterator() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygkD1ZVX0g6r"
      },
      "outputs": [],
      "source": [
        "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqelwOU50g6r"
      },
      "source": [
        "#### Using them together, with the get_next method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W4Qs_8J0g6r"
      },
      "outputs": [],
      "source": [
        "for item in number_list1_dataset:\n",
        "    number = iterator.get_next().numpy()\n",
        "    print(number)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVt6SVeV0g6r"
      },
      "source": [
        ">Note that executing this code twice in the same program run will raise an error because we are using a one-shot iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTwEvP-M0g6r"
      },
      "source": [
        "#### It's also possible to access the data in batches() with the batch method. Note that the first argument is the number of elements to put in each batch and the second is the self-explanatory drop_remainder argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C35WVPtX0g6r"
      },
      "outputs": [],
      "source": [
        "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1).batch(3, drop_remainder = False)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)\n",
        "for item in number_list1_dataset:\n",
        "    number = iterator.get_next().numpy()\n",
        "    print(number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "726SkFoB0g6r"
      },
      "source": [
        "### There is also a zip method, which is useful for presenting features and labels together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlHIRLfD0g6r"
      },
      "outputs": [],
      "source": [
        "data_set1 = [1,2,3,4,5]\n",
        "data_set2 = ['a','e','i','o','u']\n",
        "data_set1 = tf.data.Dataset.from_tensor_slices(data_set1)\n",
        "data_set2 = tf.data.Dataset.from_tensor_slices(data_set2)\n",
        "zipped_datasets = tf.data.Dataset.zip((data_set1, data_set2))\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
        "for item in zipped_datasets:\n",
        "    number = iterator.get_next()\n",
        "    print(number)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-14lrLf0g6r"
      },
      "source": [
        "#### We can concatenate two datasets as follows, using the concatenate method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4Qhn8BM0g6w"
      },
      "outputs": [],
      "source": [
        "datas1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
        "datas2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
        "datas3 = datas1.concatenate(datas2)\n",
        "print(datas3)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(datas3)\n",
        "for i in range(14):\n",
        "    number = iterator.get_next()\n",
        "    print(number)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFrb-Qmv0g6w"
      },
      "source": [
        "#### We can also do away with iterators altogether, as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-YVzhCV0g6w"
      },
      "outputs": [],
      "source": [
        "epochs=2\n",
        "for e in range(epochs):\n",
        "    for item in datas3:\n",
        "        print(item)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
