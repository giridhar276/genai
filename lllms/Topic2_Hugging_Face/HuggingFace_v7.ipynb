{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDs9DRHeU0lA"
   },
   "source": [
    "##Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UkbG0gAAOzf"
   },
   "outputs": [],
   "source": [
    "#!pip3 -q install accelerate -U\n",
    "#!pip3 -q install transformers\n",
    "#!pip3 -q install datasets\n",
    "#Restart after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fo9M10uMU5Ea"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUKTA2TCK0Bm"
   },
   "source": [
    "# Hugging Face models - Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owPpa22P79T4"
   },
   "source": [
    "## Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqbx0s86EKjR"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hasDCZe3EKr5"
   },
   "outputs": [],
   "source": [
    "senti_model = pipeline(task=\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o9ehxYhEKuy"
   },
   "outputs": [],
   "source": [
    "senti_model(\"This movie is damn good. I loved it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLm9YvnHEKx7"
   },
   "outputs": [],
   "source": [
    "senti_model(\"This is a bad phone. The screen and battery are of poor quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUr-TS8_8AMH"
   },
   "source": [
    "## Sentiment Analysis Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hno4ArGtACMZ"
   },
   "outputs": [],
   "source": [
    "Senti_model_2 = pipeline(task=\"sentiment-analysis\",\n",
    "                         model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_f-HrS2AUqW"
   },
   "outputs": [],
   "source": [
    "Senti_model_2(\"Over heating issue don't by this product camera was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Senti_model_2(\"Over heating issue don't by this product camera was good\")\n",
    "output[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTVT8t5AAusv"
   },
   "outputs": [],
   "source": [
    "Senti_model_2(\"Waste of money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ajo60hlA34F"
   },
   "outputs": [],
   "source": [
    "Senti_model_2(\"Good but manufacturing date is of feb 2025, looks like shopkeeper have old stock and he or she through on me to get rid of old stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnHzKrVbOcB0"
   },
   "source": [
    "## Prediction on your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3xBKkh9Ohoy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user_review_data=pd.read_csv(\"https://raw.githubusercontent.com/giridhar276/Datasets/master/Amazon_Yelp_Reviews/Review_Data.csv\")\n",
    "user_review_data=user_review_data.sample(50)\n",
    "user_review_data[\"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orvgbqqMOhr6"
   },
   "outputs": [],
   "source": [
    "user_review_data[\"Predicted_Sentiment\"] = user_review_data[\"Review\"].apply(lambda x: Senti_model_2(x)[0][\"label\"])\n",
    "user_review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBFCPGdZpCHm"
   },
   "source": [
    "## Load the model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKL7cOZbUvHR"
   },
   "outputs": [],
   "source": [
    "Senti_model_2_gpu = pipeline(task=\"sentiment-analysis\",\n",
    "                         model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLG1kfWYWJYR"
   },
   "outputs": [],
   "source": [
    "user_review_data[\"Predicted_Sentiment\"] = user_review_data[\"Review\"].apply(lambda x: Senti_model_2_gpu(x)[0][\"label\"])\n",
    "user_review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XuiFIE2lP3F"
   },
   "source": [
    "## Language Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOD9eWGClMHd"
   },
   "outputs": [],
   "source": [
    "translator_model = pipeline(task=\"translation_en_to_fr\",\n",
    "                            model=\"google-t5/t5-small\")\n",
    "translator_model(\"My name is Giri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XFeJM7wpfGj"
   },
   "outputs": [],
   "source": [
    "#Clear the cache in GPU\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrFOXzUCtj9D"
   },
   "source": [
    "## Question and Answer Based on a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrQG06JItsjz"
   },
   "outputs": [],
   "source": [
    "qa_model = pipeline(task=\"question-answering\",\n",
    "                    model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsAHxf1xyNZA"
   },
   "outputs": [],
   "source": [
    "# If you get any locale related error\n",
    "'''\n",
    "import locale\n",
    "print(locale.getpreferredencoding())\n",
    "\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJYJJ2upt2r6"
   },
   "outputs": [],
   "source": [
    "#Importing computer_scientists.txt document from github\n",
    "#!wget https://raw.githubusercontent.com/giridhar276/Datasets/master/computer_scientists/computer_scientists.txt\n",
    "document=open(\"computer_scientists.txt\").read()\n",
    "\n",
    "# if your area of interest is pdf ... get the pdf here\n",
    "#import pypdf2\n",
    "#import pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3H2_V_6t60E"
   },
   "outputs": [],
   "source": [
    "qa_model({'question':\"Who is the first computer programmer?\",\n",
    "          'context':document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWnKev1hvk6h"
   },
   "outputs": [],
   "source": [
    "qa_model({'question':\"What did Yann LeCun contribute?\",\n",
    "          'context':document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTT84KvAz81F"
   },
   "outputs": [],
   "source": [
    "qa_model({'question':\"Who is the father of deep learning?\",\n",
    "          'context':document})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k91FClC4rUhC"
   },
   "source": [
    "## NER (Name Entity Recognition) Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAJDsLvs3ctv"
   },
   "outputs": [],
   "source": [
    "ner_model = pipeline(task=\"ner\",\n",
    "                     model=\"dslim/bert-base-NER\",\n",
    "                     aggregation_strategy=\"simple\")\n",
    "#aggregation_strategy =\"Simple\" ; simplifies the output and makes it easy to read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcYMkTmQ3gu2"
   },
   "outputs": [],
   "source": [
    "sample_doc=\"\"\"\n",
    "Hello,\n",
    "  I, John Smith, a member of the Tech Innovators team, would like to schedule a meeting with you,\n",
    "  Mary Johnson, from the Quantum Solutions group, on Tuesday, February 8th, 2024, at 10:00 AM.\n",
    "  We can meet at your office in San Francisco or, if more convenient, at the Cafe Bella in New York City.\n",
    "  Please let me know if this date and time work for you and I am using IP address 3.3.3.3 and using email id giridhar276@gmail.com\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bvn5Vlwv5L7P"
   },
   "outputs": [],
   "source": [
    "entities = ner_model(sample_doc)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgQMV9UQ5rlS"
   },
   "outputs": [],
   "source": [
    "# Convert the above output into a dataframe and print it with the entity name\n",
    "NER_result = pd.DataFrame(entities, columns=[\"word\", \"entity_group\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(NER_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lrcBe5ejRV9"
   },
   "source": [
    "## Text Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA5GowuK6TCR"
   },
   "outputs": [],
   "source": [
    "summarizer_model = pipeline(task=\"summarization\",\n",
    "                            model=\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z-8rQXe6bfF"
   },
   "outputs": [],
   "source": [
    "Book_essay = \"\"\"\n",
    "The 7 Habits of Highly Effective People\" is a timeless self-help book by Stephen R. Covey that offers a holistic approach to personal and professional effectiveness. The book is a guide to transforming one's life by adopting seven fundamental habits.\n",
    "Covey's philosophy centers on the idea that true success is achieved by aligning one's values with principles that govern human effectiveness. The first three habits focus on personal development, emphasizing the importance of taking control of one's life, setting clear goals, and prioritizing tasks based on importance rather than urgency.\n",
    "The next three habits delve into the concept of interdependence, emphasizing the significance of effective communication, cooperation, and collaboration in achieving mutually beneficial outcomes. Covey argues that fostering strong interpersonal relationships and empathetic listening are key to building trust and synergy.\n",
    "The seventh habit, \"Sharpen the Saw,\" encourages continuous self-renewal and personal growth through physical, mental, emotional, and spiritual well-being.\n",
    "Throughout the book, Covey provides practical advice and real-life examples to illustrate each habit's application in various aspects of life, from family and work to leadership and community involvement. \"The 7 Habits of Highly Effective People\" has had a profound impact on individuals seeking personal and professional growth, offering a framework for achieving lasting success and a sense of fulfillment..\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_EDurlV7u9Q"
   },
   "outputs": [],
   "source": [
    "print(summarizer_model(Book_essay, max_length=120, min_length=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq-viv3QDAT_"
   },
   "source": [
    "## Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9jLaxAj7_Nz"
   },
   "outputs": [],
   "source": [
    "text_generator_model = pipeline(task=\"text-generation\",\n",
    "                                model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYPSOfJlDLvo"
   },
   "outputs": [],
   "source": [
    "# Generate text starting with the given prompt\n",
    "text_result = text_generator_model(\"The best way to start a presentation is\")\n",
    "print(text_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlqySLbBv1rD"
   },
   "source": [
    "# Hugging Face models without pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMErYtwhK-CV"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAtrFzinDMqI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I3a2q9L23ui"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "raw_text = \"This is a great book\"\n",
    "encoded_input = tokenizer(raw_text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits.detach().numpy()\n",
    "y_pred = np.argmax(logits)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jISpOd5j1WNY"
   },
   "outputs": [],
   "source": [
    "#Code for passing multiple examples to the above model\n",
    "\n",
    "import numpy as np\n",
    "# Prepare the input texts\n",
    "texts = [\n",
    "    \"This is a great book\",\n",
    "    \"The food was not tasty and it was very cold\",\n",
    "    \"The weather is very good today\",\n",
    "]\n",
    "\n",
    "# Tokenize and encode the input texts\n",
    "encoded_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the encoded inputs to the model\n",
    "outputs = model(**encoded_inputs)\n",
    "\n",
    "# Get the model's predictions\n",
    "logits = outputs.logits.detach().cpu().numpy()\n",
    "\n",
    "# Find the predicted class for each input\n",
    "predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EbNv3bmBF7V"
   },
   "source": [
    "# Finetuning HuggingFace model\n",
    "Code Explanation- [Click here](https://github.com/venkatareddykonasani/Assorted/blob/main/Fine_tuning_HuggingFace.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYFE5O8RB4MD"
   },
   "source": [
    "### Bank Complaints Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGkqrKxCfrHy"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/giridhar276/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip\n",
    "#!unzip -o complaints_v2.zip\n",
    "#complaints_data = pd.read_csv(\"/content/complaints_v2.csv\")\n",
    "#complaints_data.head()\n",
    "\n",
    "\n",
    "##!curl -LO https://github.com/giridhar276/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip\n",
    "#!unzip -o complaints_v2.zip\n",
    "\n",
    "\n",
    "complaints_data = pd.read_csv(\"complaints_v2.csv\")\n",
    "complaints_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqmjOcoOB-ec"
   },
   "source": [
    "### Use distilbert model without finetunung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BJkxMer_wjr"
   },
   "outputs": [],
   "source": [
    "# Distil bert model\n",
    "from transformers import pipeline\n",
    "distilbert_model = pipeline(task=\"text-classification\",\n",
    "                            model=\"distilbert-base-uncased\",\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model(\" mortgage well fargo bank since meet conditi...\")[0]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WbHzWMRDiRW"
   },
   "outputs": [],
   "source": [
    "sample_data=complaints_data.sample(100, random_state=42)\n",
    "sample_data[\"text\"]=sample_data[\"text\"].apply(lambda x: \" \".join(x.split()[:350]))\n",
    "sample_data[\"bert_predicted\"] = sample_data[\"text\"].apply(lambda x: distilbert_model(x)[0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default prediction is not a number LABEL_1, LABEL_0\n",
    "sample_data[\"bert_predicted_num\"]=sample_data[\"bert_predicted\"].apply(lambda x: x[-1])\n",
    "sample_data[\"bert_predicted_num\"] = sample_data[\"bert_predicted_num\"].astype(int)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRx_9gXrklH4"
   },
   "source": [
    "### Accuracy of the model without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_LxeuwUIg6r"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sample_data[\"label\"], sample_data[\"bert_predicted_num\"])\n",
    "print(cm)\n",
    "accuracy=cm.diagonal().sum()/cm.sum()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IoArWg3ksvC"
   },
   "source": [
    "## Project - Finetuning the model with our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvhIoOLQBG8I"
   },
   "outputs": [],
   "source": [
    "#!pip -q install accelerate -U\n",
    "#!pip -q install transformers[torch]\n",
    "#!pip -q install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZT4RxigoO3a"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict, ClassLabel, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF1wh7rY4MtP"
   },
   "outputs": [],
   "source": [
    "#The target variable must be named as \"label\" - Verify it, before proceeding\n",
    "print(sample_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXivZhOxjQtA"
   },
   "outputs": [],
   "source": [
    "Sample_data = Dataset.from_pandas(sample_data)\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = Sample_data.train_test_split(test_size=0.2)  # 80% training, 20% testing\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYxjyEgRR2p-"
   },
   "source": [
    "### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khB_bZv0lcXL"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'} )\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Q_vJOISAtX"
   },
   "source": [
    "### Load and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcBw9CWmf0_D"
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                            num_labels=2,\n",
    "                                                            pad_token_id=tokenizer.eos_token_id) # Adjust num_labels as needed\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbMzSADKvCtB"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_bert_custom\",\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs_bert_custom\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIVONC0H7YZc"
   },
   "outputs": [],
   "source": [
    "# Define the directory where you want to save your model and tokenizer\n",
    "model_dir = \"./distilbert_finetuned\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "#Save the model with\n",
    "trainer.save_model('Distilbert_CustomModel_10K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5zPdvspOrjc"
   },
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "  new_complaint=text\n",
    "  inputs=tokenizer(new_complaint, return_tensors=\"pt\")\n",
    "  inputs = inputs.to(torch.device(\"mps\"))   # either mps or cpu  or cuda:0\n",
    "  outputs=model(**inputs)\n",
    "  predictions=outputs.logits.argmax(-1)\n",
    "  predictions=predictions.detach().cpu().numpy()\n",
    "  return(predictions)\n",
    "\n",
    "sample_data[\"finetuned_predicted\"]=sample_data[\"text\"].apply(lambda x: make_prediction(str(x))[0])\n",
    "sample_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8BsIrTyWc72"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create the confusion matrix\n",
    "cm1 = confusion_matrix(sample_data[\"label\"], sample_data[\"finetuned_predicted\"])\n",
    "print(cm1)\n",
    "accuracy1=cm1.diagonal().sum()/cm1.sum()\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwrPpOpzawh"
   },
   "source": [
    "### Loading a pre-built model and making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmhTs6AAzFDI"
   },
   "outputs": [],
   "source": [
    "#Code to donwloading the distilbert model\n",
    "!gdown --id 1785J3ir19RaZP3ebbFvWUX88PMaBouro -O distilbert_finetuned_V1.zip\n",
    "!unzip -o -j distilbert_finetuned_V1.zip -d distilbert_finetuned_V1\n",
    "\n",
    "model_v1 = DistilBertForSequenceClassification.from_pretrained('./distilbert_finetuned_V1')\n",
    "model_v1.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCJz8FD99xcK"
   },
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "  new_complaint=text\n",
    "  inputs=tokenizer(new_complaint, return_tensors=\"pt\")\n",
    "  inputs = inputs.to(torch.device(\"mps\"))\n",
    "  outputs=model_v1(**inputs)\n",
    "  predictions=outputs.logits.argmax(-1)\n",
    "  predictions=predictions.detach().cpu().numpy()\n",
    "  return(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99Z7s9P-C-hg"
   },
   "outputs": [],
   "source": [
    "sample_data_large=complaints_data.sample(n=1000, random_state=55)\n",
    "sample_data_large[\"finetuned_predicted\"]=sample_data_large[\"text\"].apply(lambda x: make_prediction(str(x)[:350])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLAOpceYfkWI"
   },
   "outputs": [],
   "source": [
    "sample_data_large[\"finetuned_predicted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9Iq2KJD-AJ5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create the confusion matrix\n",
    "cm1 = confusion_matrix(sample_data_large[\"label\"], sample_data_large[\"finetuned_predicted\"])\n",
    "print(cm1)\n",
    "accuracy1=cm1.diagonal().sum()/cm1.sum()\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vm1l0v6Dx-jb"
   },
   "source": [
    "# Saving the Model on HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6SPRqIiynQW"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install -U ipykernel #for executing the commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcqXDgoizIyD"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gmtZkvqDJNV"
   },
   "outputs": [],
   "source": [
    "!pip3 install gdown\n",
    "!gdown --id 1785J3ir19RaZP3ebbFvWUX88PMaBouro -O distilbert_finetuned_V1.zip\n",
    "!unzip -o -j distilbert_finetuned_V1.zip -d distilbert_finetuned_V1\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('./distilbert_finetuned_V1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJmy5o-I0SGn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs-qSjk-z0dd"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#To get Auth token: Profile >> Settings >>Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRPRvUh-0esS"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"giridhar276/Bank_distil_bert_10K_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFnoFMsHBEw8"
   },
   "source": [
    "# Loading the model from HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FbKUERK0k5p"
   },
   "outputs": [],
   "source": [
    "model=DistilBertForSequenceClassification.from_pretrained(\"giridhar276/Bank_distil_bert_10K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsKSd1EM1DS1"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGd2quFz1Hss"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!wget https://github.com/giridhar276/Datasets/raw/master/Bank_Customer_Complaints/complaints_v2.zip\n",
    "#!unzip -o complaints_v2.zip\n",
    "complaints_data = pd.read_csv(\"./complaints_v2.csv\")\n",
    "list(complaints_data[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyRrPXtF2YFe"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_2szgSb1rcc"
   },
   "outputs": [],
   "source": [
    "complaint=\"\"\"\n",
    "payment history missing credit report made mistake put account forbearance without authorization knowledge matter fact automatic payment setup month monthly mortgage paid full noticed issue account marked forbearance credit report tried get new home loan another new bank contacted immediately asked fix error provide letter detail please see asks forbearance issue seemed fixed however credit report payment history missing new bank able approve new loan issue missing payment history contacted time since phone ask thing report payment history transunion fix missing data issue provide letter show account never forbearance payment history past month however waiting week countless email phone call talk multiple supervisor able get either one thing without issue fixed new bank process new loan application therefore need help immediately get fixed\n",
    "\"\"\"\n",
    "\n",
    "inputs=tokenizer(complaint, return_tensors=\"pt\")\n",
    "outputs=model(**inputs)\n",
    "predictions=outputs.logits.argmax(-1)\n",
    "predictions=predictions.detach().cpu().numpy()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTFBcRMfBM6W"
   },
   "source": [
    "# Web App Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIkhfFOwkLY7"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "streamlit\n",
    "numpy\n",
    "pandas\n",
    "torch\n",
    "transformers\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Opz4IsR8mOH8"
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZYun4Kf2UkY"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('giridhar2761111/Bank_distil_bert_10K')\n",
    "\n",
    "st.title(\"Bank Complaints Categorization\")\n",
    "st.write(\"Sample Complaints are given below\")\n",
    "Sample_Complaints = [\n",
    "    {\"Sentence\": \"Credit Report - payment history missing credit report made mistake put account forbearance without authorization \"},\n",
    "    {\"Sentence\": \"Retail Related - forwarded message cc sent friday pdt subject final legal payment well fargo well fargo clearly wrong need look actually opened account see court hearing several different government agency \"}\n",
    "]\n",
    "st.table(Sample_Complaints)\n",
    "user_input = st.text_input(\"Enter a complaint:\")\n",
    "button=st.button(\"Classify\")\n",
    "\n",
    "d={\n",
    "    0: \"Credit reporting\",\n",
    "    1: \"Mortgage and Others\"\n",
    "}\n",
    "\n",
    "if user_input and button:\n",
    "  inputs=tokenizer(user_input, return_tensors=\"pt\")\n",
    "  outputs=model(**inputs)\n",
    "  predictions=outputs.logits.argmax(-1)\n",
    "  predictions=predictions.detach().cpu().numpy()\n",
    "  print(predictions)\n",
    "  st.write(\"Prediction :\" , d[predictions[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIaHczsX-wT0"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n",
    "\n",
    "#This sometimes doesn't work on Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app1.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "st.title(\"ðŸ“„ Excel File Uploader and Viewer\")\n",
    "\n",
    "# Upload Excel file\n",
    "uploaded_file = st.file_uploader(\"Choose an Excel file\", type=[\"xlsx\", \"xls\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(uploaded_file)\n",
    "        \n",
    "        # Show dataframe\n",
    "        st.success(\"File uploaded successfully!\")\n",
    "        st.subheader(\"Preview of the uploaded Excel file:\")\n",
    "        st.dataframe(df)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error reading the Excel file: {e}\")\n",
    "else:\n",
    "    st.info(\"Please upload an Excel file to proceed.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPrXTTVq6d23irLtTksFhhh",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1g583Kvn4EAGwKflKQIwkfJty-jK9FeSc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
