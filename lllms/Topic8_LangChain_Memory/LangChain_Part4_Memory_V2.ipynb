{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb172S4aDFKS"
   },
   "outputs": [],
   "source": [
    "!pip install openai --quiet\n",
    "!pip install langchain --quiet\n",
    "!pip install cohere --quiet\n",
    "!pip install tiktoken --quiet\n",
    "!pip install langchain_community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iE1mMtv1C5NM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "#os.environ['COHERE_API_KEY'] = \"Your own COHERE_API_KEY\"\n",
    "\n",
    "#Better way\n",
    "#from google.colab import userdata\n",
    "#os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
    "#os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pY6qEt4kY4gZ"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btiPFnoDK5yp"
   },
   "source": [
    "# Memory in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BO6wD-TLYVPJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/4204959233.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm= OpenAI(temperature=0, max_tokens=512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "India won the t20 cricket World Cup in 2007.\n",
      "\n",
      "\n",
      "1. Virat Kohli (India)\n",
      "2. Babar Azam (Pakistan)\n",
      "3. David Warner (Australia)\n",
      "4. Kane Williamson (New Zealand)\n",
      "5. Eoin Morgan (England)\n",
      "6. Chris Gayle (West Indies)\n",
      "7. Rashid Khan (Afghanistan)\n",
      "8. Lasith Malinga (Sri Lanka)\n",
      "9. Shakib Al Hasan (Bangladesh)\n",
      "10. Quinton de Kock (South Africa)\n",
      "11. Mohammad Nabi (Afghanistan)\n",
      "12. Jasprit Bumrah (India)\n",
      "13. Mitchell Starc (Australia)\n",
      "14. Trent Boult (New Zealand)\n",
      "15. Jofra Archer (England)\n"
     ]
    }
   ],
   "source": [
    "llm= OpenAI(temperature=0, max_tokens=512)\n",
    "\n",
    "print(llm.invoke(\"When did India win the t20 cricket World Cup? \"))\n",
    "print(llm.invoke(\"Give me some players in that team t20 cricket World Cup \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Hi Venkat, my name is [Your Name] and I am happy to assist you with your order. Can you please provide me with your order number so I can look into it for you?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Hello, My Name is Venkat. I am from Bangalore. I need help with my order\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypy4zF3R8D00"
   },
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gzOa5W4171qX"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fSRIYudo8f8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/64278653.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm= ChatOpenAI(temperature=0, max_tokens=512)\n",
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/64278653.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/64278653.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India won the ICC T20 Cricket World Cup in 2007, which was the inaugural edition of the tournament. They defeated Pakistan in the final to claim the title.\n",
      "Some key players in the 2007 ICC T20 Cricket World Cup-winning Indian team included MS Dhoni (captain), Yuvraj Singh, Gautam Gambhir, Virender Sehwag, Harbhajan Singh, and RP Singh.\n"
     ]
    }
   ],
   "source": [
    "llm= ChatOpenAI(temperature=0, max_tokens=512)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm,\n",
    "                                 memory=memory,\n",
    "                                 verbose=False\n",
    "                                 )\n",
    "print(conversation.predict(input=\"When did India win the t20 cricket World Cup? \"))\n",
    "print(conversation.predict(input=\"Give me some players in that team\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpHLvo5ZPC6V"
   },
   "source": [
    "## With Verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kEDfyccEPFsd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: When did India win the t20 cricket World Cup? \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "India won the ICC T20 Cricket World Cup in 2007 and 2011. They defeated Pakistan in the final in 2007 and Sri Lanka in the final in 2011.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: When did India win the t20 cricket World Cup? \n",
      "AI: India won the ICC T20 Cricket World Cup in 2007 and 2011. They defeated Pakistan in the final in 2007 and Sri Lanka in the final in 2011.\n",
      "Human: Give me some players in that team\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Some players in the 2007 and 2011 ICC T20 Cricket World Cup-winning Indian teams were MS Dhoni, Yuvraj Singh, Gautam Gambhir, Virender Sehwag, Harbhajan Singh, Zaheer Khan, and Suresh Raina.\n"
     ]
    }
   ],
   "source": [
    "llm= ChatOpenAI(temperature=0, max_tokens=512)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm,\n",
    "                                 memory=memory,\n",
    "                                 verbose=True\n",
    "                                 )\n",
    "print(conversation.predict(input=\"When did India win the t20 cricket World Cup? \"))\n",
    "print(conversation.predict(input=\"Give me some players in that team\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: what are the revenues of oracle in this quarter\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oracle reported revenues of $9.73 billion for the most recent quarter, which was a 3% increase from the same quarter last year.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"what are the revenues of oracle in this quarter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o810g19hW9HU"
   },
   "source": [
    "### What happens behind the scenes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WZIZ074aW0sV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: When did India win the t20 cricket World Cup? \n",
      "AI: India won the ICC T20 Cricket World Cup in 2007 and 2011. They defeated Pakistan in the final in 2007 and Sri Lanka in the final in 2011.\n",
      "Human: Give me some players in that team\n",
      "AI: Some players in the 2007 and 2011 ICC T20 Cricket World Cup-winning Indian teams were MS Dhoni, Yuvraj Singh, Gautam Gambhir, Virender Sehwag, Harbhajan Singh, Zaheer Khan, and Suresh Raina.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qlz4E-CPGTd"
   },
   "source": [
    "## Memory Example-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VhTgmXyuOmBh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! The main differences between Classification and Regression are:\\n\\n1. Output: In Classification, the output variable is a category or a class label, while in Regression, the output variable is a continuous value.\\n\\n2. Goal: The goal of Classification is to predict the class label of a new instance based on its features, while the goal of Regression is to predict a continuous value for a new instance.\\n\\n3. Evaluation: In Classification, evaluation metrics like accuracy, precision, recall, and F1 score are used to measure the performance of the model, while in Regression, evaluation metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared are used.\\n\\nThese are the three main differences between Classification and Regression. Let me know if you have any more questions!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm= ChatOpenAI(temperature=0, max_tokens=512)\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm,\n",
    "                                 memory=memory,\n",
    "                                 #verbose=True\n",
    "                                 )\n",
    "conversation.predict(input=\"Explain three main differences Classification and Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zEdXZrc49XdV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here is the information in markdown tabular code format:\\n\\n| Difference      | Classification                               | Regression                                   |\\n|-----------------|----------------------------------------------|----------------------------------------------|\\n| Output          | Category or class label                      | Continuous value                             |\\n| Goal            | Predict class label of new instance          | Predict continuous value for new instance   |\\n| Evaluation      | Accuracy, precision, recall, F1 score        | Mean Squared Error, RMSE, R-squared          |'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Give the above output in markdown tabular code format \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjMfwfGzAc7o"
   },
   "source": [
    "  | Main Differences | Classification | Regression |\n",
    "| --- | --- | --- |\n",
    "| Type of Learning | Supervised | Supervised |\n",
    "| Predicted Values | Categorical or discrete | Continuous numerical |\n",
    "| Techniques | Decision trees, support vector machines, neural networks | Linear regression, logistic regression, polynomial regression |\n",
    "| Evaluation Metrics | Accuracy, precision, recall | Mean squared error, mean absolute error, R-squared |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uUcAWphy9mGx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification is most widely used in predicting customer churn. This is because customer churn prediction involves predicting whether a customer will stay or leave, which can be framed as a classification problem where the output is a binary class label (e.g., churn or not churn). Classification algorithms are commonly used in this scenario to classify customers into these two categories based on their features and behavior.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Which one is most widely used in predicting customer churn?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sCKFcBDI90xL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Another difference between Classification and Regression is the type of algorithms used. In Classification, algorithms like Logistic Regression, Decision Trees, Random Forest, Support Vector Machines (SVM), and Neural Networks are commonly used. In Regression, algorithms like Linear Regression, Polynomial Regression, Lasso Regression, Ridge Regression, and Support Vector Regression (SVR) are commonly used. These algorithms are specifically designed to handle the different types of output variables and goals of each type of problem.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Give me one more difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "s6hvFhF6BRK2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Explain three main differences Classification and Regression\n",
      "AI: Sure! The main differences between Classification and Regression are:\n",
      "\n",
      "1. Output: In Classification, the output variable is a category or a class label, while in Regression, the output variable is a continuous value.\n",
      "\n",
      "2. Goal: The goal of Classification is to predict the class label of a new instance based on its features, while the goal of Regression is to predict a continuous value for a new instance.\n",
      "\n",
      "3. Evaluation: In Classification, evaluation metrics like accuracy, precision, recall, and F1 score are used to measure the performance of the model, while in Regression, evaluation metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared are used.\n",
      "\n",
      "These are the three main differences between Classification and Regression. Let me know if you have any more questions!\n",
      "Human: Give the above output in markdown tabular code format \n",
      "AI: Sure! Here is the information in markdown tabular code format:\n",
      "\n",
      "| Difference      | Classification                               | Regression                                   |\n",
      "|-----------------|----------------------------------------------|----------------------------------------------|\n",
      "| Output          | Category or class label                      | Continuous value                             |\n",
      "| Goal            | Predict class label of new instance          | Predict continuous value for new instance   |\n",
      "| Evaluation      | Accuracy, precision, recall, F1 score        | Mean Squared Error, RMSE, R-squared          |\n",
      "Human: Which one is most widely used in predicting customer churn?\n",
      "AI: Classification is most widely used in predicting customer churn. This is because customer churn prediction involves predicting whether a customer will stay or leave, which can be framed as a classification problem where the output is a binary class label (e.g., churn or not churn). Classification algorithms are commonly used in this scenario to classify customers into these two categories based on their features and behavior.\n",
      "Human: Give me one more difference\n",
      "AI: Sure! Another difference between Classification and Regression is the type of algorithms used. In Classification, algorithms like Logistic Regression, Decision Trees, Random Forest, Support Vector Machines (SVM), and Neural Networks are commonly used. In Regression, algorithms like Linear Regression, Polynomial Regression, Lasso Regression, Ridge Regression, and Support Vector Regression (SVR) are commonly used. These algorithms are specifically designed to handle the different types of output variables and goals of each type of problem.\n"
     ]
    }
   ],
   "source": [
    "#What happens behind the scenes?\n",
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdSPeHmsUoJ4"
   },
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZiPXoA0YCT6x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/2173228185.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'd be happy to help you with your order. Can you please provide me with your order number so I can look up the details for you?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "conversation.predict(input=\"Hello, I need help with my order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xbkNDQChUADo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great choice! Do you have the order number handy so I can look up the details for you?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I've ordered a pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6t6jolCMUIyz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No problem! Do you remember when you placed the order or any specific toppings or size you requested? That might help me narrow down the search for your order.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I don't remember the order details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "MsK75HS8UgDB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human asks for help with their order, specifically a pizza. The AI offers to assist and asks for the order number to look up the details, praising the choice of pizza. The human doesn't remember the order details, but the AI offers to help by asking about the type of pizza or specific toppings to try and locate the order.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7UbWDAsP-gI"
   },
   "source": [
    "## ConverstationEntitySummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IgEGRiTfRUHk"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
    "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DCk4YwWPR5SE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{entities}\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Last line:\n",
      "Human: {input}\n",
      "You:\n"
     ]
    }
   ],
   "source": [
    "template=ENTITY_MEMORY_CONVERSATION_TEMPLATE.template\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_x7qSkN7QBqa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/86748072.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationEntityMemory(llm=llm)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pydantic/main.py:212: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Venkat': '', 'Bangalore': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: Hello, My Name is Venkat. I am from Bangalore. I need help with my order\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Venkat! I'm here to help you with your order. Please provide me with more details about your order so I can assist you better.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationEntityMemory(llm=llm)\n",
    "conversation = ConversationChain(llm=llm,\n",
    "                                 memory=memory,\n",
    "                                 verbose=True,\n",
    "                                 prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE)\n",
    "conversation.predict(input=\"Hello, My Name is Venkat. I am from Bangalore. I need help with my order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h4aPc45rRB0F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{}\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, My Name is Venkat. I am from Bangalore. I need help with my order\n",
      "AI: Hello Venkat! I'm here to help you with your order. Please provide me with more details about your order so I can assist you better.\n",
      "Last line:\n",
      "Human: I ordered Mobile phone on tuesday night 8pm\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thank you for providing that information, Venkat. Can you please let me know what specific help you need with your order for the mobile phone you placed on Tuesday night at 8pm?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I ordered Mobile phone on tuesday night 8pm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jqgoqGd_S0lG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'IBLC19678456': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, My Name is Venkat. I am from Bangalore. I need help with my order\n",
      "AI: Hello Venkat! I'm here to help you with your order. Please provide me with more details about your order so I can assist you better.\n",
      "Human: I ordered Mobile phone on tuesday night 8pm\n",
      "AI: Thank you for providing that information, Venkat. Can you please let me know what specific help you need with your order for the mobile phone you placed on Tuesday night at 8pm?\n",
      "Last line:\n",
      "Human: My order number is IBLC19678456. What is the status of my order?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Let me check the status of your order with the order number IBLC19678456. Give me a moment to look it up for you.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My order number is IBLC19678456. What is the status of my order?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "YXYMQGG7S2mQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, My Name is Venkat. I am from Bangalore. I need help with my order', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\" Hello Venkat, it's nice to meet you. I am here to assist you with your order. Can you please provide me with more details about your order so I can better understand how to help you?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I ordered Mobile phone on tuesday night 8pm', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=' Thank you for providing me with that information. Can you please tell me which specific mobile phone you ordered and from which website or store? This will help me provide you with more accurate assistance.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My order number is IBLC19678456. What is the status of my order?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=' Thank you for providing me with your order number. Let me check the status of your order for you. It looks like your order is currently being processed and is expected to be shipped within the next 24 hours. Is there anything else I can assist you with?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLGNPWJaTB5G"
   },
   "outputs": [],
   "source": [
    "conversation.memory.entity_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtOyURrOcyfN"
   },
   "source": [
    "# Building a Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "I3vLanZ2Sov6"
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wk6BiZ-lc9b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI message ==> Hello! How are you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hello\n",
      "AI: Hello! How are you today?\n",
      "Human: I am not good today\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI message ==> I'm sorry to hear that. Is there anything specific that's bothering you?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hello\n",
      "AI: Hello! How are you today?\n",
      "Human: I am not good today\n",
      "AI: I'm sorry to hear that. Is there anything specific that's bothering you?\n",
      "Human: I am having fever\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI message ==> I'm sorry to hear that you're not feeling well. It's important to rest and stay hydrated when you have a fever. Have you taken any medication or seen a doctor yet?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hello\n",
      "AI: Hello! How are you today?\n",
      "Human: I am not good today\n",
      "AI: I'm sorry to hear that. Is there anything specific that's bothering you?\n",
      "Human: I am having fever\n",
      "AI: I'm sorry to hear that you're not feeling well. It's important to rest and stay hydrated when you have a fever. Have you taken any medication or seen a doctor yet?\n",
      "Human: I am having fever\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI message ==> It's important to monitor your temperature and make sure it doesn't get too high. If your fever persists or gets worse, it's best to consult a healthcare professional for further guidance. In the meantime, make sure to drink plenty of fluids and get plenty of rest. Take care of yourself!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hello\n",
      "AI: Hello! How are you today?\n",
      "Human: I am not good today\n",
      "AI: I'm sorry to hear that. Is there anything specific that's bothering you?\n",
      "Human: I am having fever\n",
      "AI: I'm sorry to hear that you're not feeling well. It's important to rest and stay hydrated when you have a fever. Have you taken any medication or seen a doctor yet?\n",
      "Human: I am having fever\n",
      "AI: It's important to monitor your temperature and make sure it doesn't get too high. If your fever persists or gets worse, it's best to consult a healthcare professional for further guidance. In the meantime, make sure to drink plenty of fluids and get plenty of rest. Take care of yourself!\n",
      "Human: \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI message ==> Is there anything else you'd like to talk about or any other questions you have? I'm here to help in any way I can.\n"
     ]
    }
   ],
   "source": [
    "#Type quit to exit this Conversation\n",
    "user_input=input(\"Your message:\")\n",
    "llm= ChatOpenAI(temperature=0, max_tokens=512)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm,\n",
    "                                 memory=memory,\n",
    "                                 verbose=True\n",
    "                                 )\n",
    "while user_input!=\"quit\":\n",
    "    print(\"AI message ==>\", conversation.predict(input=user_input))\n",
    "    user_input=input(\"Your message: \")\n",
    "while user_input ==\"quit\":\n",
    "    memory.clear()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5HjnCHJwFI9"
   },
   "source": [
    "# Example with RAG, Memory and Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWEpYYzG--Qc"
   },
   "outputs": [],
   "source": [
    "!pip install pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4C7Jewx3wlwd"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFoiLWdwCwgj"
   },
   "source": [
    "## Step-1 :Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading… https://raw.githubusercontent.com/giridhar276/Datasets/master/CIBIL_Report/All_Cibil_Docs.zip\n",
      "Saved: /Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/All_Cibil_Docs.zip - 2818524 bytes\n",
      "Extracted to: /Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/All_Docs_Set1\n",
      "Found 5 PDFs\n",
      "All_Docs_Set1/5787-15251-1-PB.pdf\n",
      "All_Docs_Set1/CIBIL Score in India.pdf\n",
      "All_Docs_Set1/MyCIBIL-Understanding.pdf\n",
      "All_Docs_Set1/creditscoretips_2.pdf\n",
      "All_Docs_Set1/understanding-cir-ctc.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import shutil, zipfile, glob\n",
    "\n",
    "ZIP_URL = \"https://raw.githubusercontent.com/giridhar276/Datasets/master/CIBIL_Report/All_Cibil_Docs.zip\"\n",
    "ZIP_PATH = Path(\"All_Cibil_Docs.zip\")\n",
    "DEST_DIR = Path(\"All_Docs_Set1\")\n",
    "\n",
    "# 1) Download the zip\n",
    "print(\"Downloading…\", ZIP_URL)\n",
    "with urlopen(ZIP_URL) as r, open(ZIP_PATH, \"wb\") as f:\n",
    "    shutil.copyfileobj(r, f)\n",
    "print(\"Saved:\", ZIP_PATH.resolve(), \"-\", ZIP_PATH.stat().st_size, \"bytes\")\n",
    "\n",
    "# 2) Extract (overwrites by default if files already exist)\n",
    "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    zf.extractall(DEST_DIR)\n",
    "print(\"Extracted to:\", DEST_DIR.resolve())\n",
    "\n",
    "# 3) List PDFs (use rglob if there may be subfolders)\n",
    "pdf_files = sorted(str(p) for p in DEST_DIR.rglob(\"*.pdf\"))\n",
    "print(\"Found\", len(pdf_files), \"PDFs\")\n",
    "for p in pdf_files:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM1yvZvewheE"
   },
   "outputs": [],
   "source": [
    "#Example of ConversationalRetrievalChain\n",
    "\n",
    "#Document Loading\n",
    "!wget https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/CIBIL_Report/All_Cibil_Docs.zip\n",
    "#Unzip and Overwrite\n",
    "!unzip -o All_Cibil_Docs.zip -d All_Docs_Set1\n",
    "#listdown all documents content/All_Docs_Set1 and store them\n",
    "pdf_files = glob.glob(\"All_Docs_Set1/*.pdf\")\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tIevEFt7-11C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All_Docs_Set1/5787-15251-1-PB.pdf 12\n",
      "All_Docs_Set1/CIBIL Score in India.pdf 7\n",
      "All_Docs_Set1/MyCIBIL-Understanding.pdf 4\n",
      "All_Docs_Set1/creditscoretips_2.pdf 1\n",
      "All_Docs_Set1/understanding-cir-ctc.pdf 3\n",
      "ee discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/351979266\n",
      "Awareness of Credit Score Mechanism in India: A Study with reference to\n",
      "Credit Information Bu reau India Limited (CIBIL)\n",
      "Article  · Mar ch 2017\n",
      "CITATIONS\n",
      "0READS\n",
      "3,857\n",
      "2 author s, including:\n",
      "S. Sank aramuthuk umar\n",
      "Bahr ain P olyt echnic\n",
      "18 PUBLICA TIONS    19 CITATIONS    \n",
      "SEE PROFILE\n",
      "All c ontent f ollo wing this p age was uplo aded b y S. Sank aramuthuk umar  on 04 F ebruar y 2023.\n",
      "The user has r equest ed enhanc ement of the do wnlo aded file. \n",
      "  1  Awareness of Credit Score Mechanism in India: A Study with reference to Credit Information Bureau India Limited (CIBIL)  Dr. Saravanan Laxmanan,  Assistant Professor, Department of Management,  College of Business & Economics, Addi\n",
      "Lines 576\n",
      "Words 10929\n",
      "Charecters 67361\n"
     ]
    }
   ],
   "source": [
    "full_text = \"\"\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load()\n",
    "    print(pdf_file, len(pages))\n",
    "    for page in pages:\n",
    "        full_text += page.page_content\n",
    "\n",
    "print(full_text[1:1000])\n",
    "print(\"Lines\" , len(full_text.split(\"\\n\")))\n",
    "print(\"Words\" , len(full_text.split(\" \")))\n",
    "print(\"Charecters\", len(full_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nddR_wGDElT"
   },
   "source": [
    "## Step-2:Split the data into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "t7vJzrSgC1Lb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/351979266\n",
      "Awareness of Credit Score Mechanism in India: A Study with reference to\n",
      "Credit Information Bu reau India Limited (CIBIL)\n",
      "Article  · Mar ch 2017\n",
      "CITATIONS\n",
      "0READS\n",
      "3,857\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=60)\n",
    "chunks = text_splitter.split_text(full_text)\n",
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciwB4WJlFrr8"
   },
   "source": [
    "## Step-3: Creating embeddings and Storing in Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YPb15teWG1Bk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ChromaDB --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ssiV5nucAQol"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/2667952954.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/2667952954.py:5: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  cibil_db.persist()\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "cibil_db=Chroma.from_texts(chunks,\n",
    "                               embeddings,\n",
    "                               persist_directory=\"cibil_db\")\n",
    "cibil_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_egFbt8UN4n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v2/fmm4xb9j1vvd6n08b20728rw0000gn/T/ipykernel_85547/3563595763.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result=retriever.get_relevant_documents(query=\"What is the CIBIL Score?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit EducationWhat is a CIBIL Score?\n",
      "CIBIL Score is a 3 digit numeric summary of your credit history, derived by using details found in the ‘Accounts’ and ‘Enquiries’\n",
      "Credit EducationWhat is a CIBIL Score?\n",
      "CIBIL Score is a 3 digit numeric summary of your credit history, derived by using details found in the ‘Accounts’ and ‘Enquiries’\n",
      "which is widely used by loan providers to evaluate loan applications. An individual’s CIBIL Score ranges between 300-900, and is calculated basis the information in the “Accounts” and “Enquiry” section of the credit report. The closer the score to 900, the more conﬁdence the loan provider will have\n",
      "which is widely used by loan providers to evaluate loan applications. An individual’s CIBIL Score ranges between 300-900, and is calculated basis the information in the “Accounts” and “Enquiry” section of the credit report. The closer the score to 900, the more conﬁdence the loan provider will have\n"
     ]
    }
   ],
   "source": [
    "retriever = cibil_db.as_retriever()\n",
    "result=retriever.get_relevant_documents(query=\"What is the CIBIL Score?\")\n",
    "for i in range(len(result)):\n",
    "  print(result[i].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk2tQsfsFyrs"
   },
   "source": [
    "## Step-4: Conversation and Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "tqMgT-PxJLc2"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI, cohere\n",
    "from langchain.llms import Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kLrb6fpwFpk6"
   },
   "outputs": [],
   "source": [
    "llm= ChatOpenAI(temperature=0, max_tokens=512)\n",
    "#llm=Cohere(temperature=0, max_tokens=512)\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "conversational_RAG = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                                     retriever=cibil_db.as_retriever(),\n",
    "                                                     memory=memory,\n",
    "                                                     #verbose=True\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr8Bc1ySIIQU"
   },
   "source": [
    "## Step-5 : Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "qqOdnx80FpoT"
   },
   "outputs": [],
   "source": [
    "user_input=input(\"Your message:\")\n",
    "while user_input!=\"quit\":\n",
    "    response=conversational_RAG({\"question\": user_input})\n",
    "    print(\"AI message ==>\", response[\"answer\"])\n",
    "    user_input=input(\"Your message: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSjFoQYdSCJo"
   },
   "source": [
    "# RAG ChatBotTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0lEI8A0QjWO"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain\n",
    "langchain_community\n",
    "PyPDF2\n",
    "python-dotenv\n",
    "streamlit\n",
    "openai\n",
    "faiss-cpu\n",
    "altair\n",
    "tiktoken\n",
    "huggingface-hub\n",
    "InstructorEmbedding\n",
    "sentence-transformers\n",
    "pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXiNMQMJTezd"
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Lb0BtRnzTkc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_text_chunks(text):\n",
    "  text_splitter = CharacterTextSplitter(\n",
    "      separator=\"\\n\",\n",
    "      chunk_size=1000,\n",
    "      chunk_overlap=200,\n",
    "      length_function=len\n",
    "  )\n",
    "  chunks = text_splitter.split_text(text)\n",
    "  return chunks\n",
    "\n",
    "def get_embeddings(text_chunks):\n",
    "  embeddings = OpenAIEmbeddings()\n",
    "  return embeddings\n",
    "\n",
    "def get_convesrational_chain(vectorstore):\n",
    "  llm = ChatOpenAI(temperature=0)\n",
    "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "  convesrational_chain = ConversationalRetrievalChain.from_llm(\n",
    "      llm=llm,\n",
    "      retriever=vectorstore.as_retriever(),\n",
    "      memory=memory\n",
    "  )\n",
    "  return convesrational_chain\n",
    "\n",
    "\n",
    "def handle_userinput(user_question):\n",
    "  response = st.session_state.convesrational_chain({\"question\": user_question})\n",
    "  st.session_state.chat_history = response['chat_history']\n",
    "\n",
    "  for i, message in enumerate(st.session_state.chat_history):\n",
    "    if i % 2 == 0 :\n",
    "      st.write(':man_in_tuxedo:', message.content)\n",
    "    else:\n",
    "      st.write(':robot_face:', message.content)\n",
    "\n",
    "def main():\n",
    "\n",
    "    st.set_page_config(page_title=\"Chat with Documents\", page_icon=\":books:\")\n",
    "    styl = f\"\"\"\n",
    "      <style>\n",
    "          .stTextInput {{\n",
    "            position: fixed;\n",
    "            bottom: 3rem;\n",
    "          }}\n",
    "      </style>\n",
    "      \"\"\"\n",
    "    st.markdown(styl, unsafe_allow_html=True)\n",
    "\n",
    "    #st.write(css, unsafe_allow_html=True)\n",
    "    st.title(\"AI Chat-Bot for Your Documents :books:\")\n",
    "\n",
    "    if \"convesrational_chain\" not in st.session_state:\n",
    "        st.session_state.convesrational_chain = None\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = None\n",
    "\n",
    "\n",
    "    user_input=st.text_input(\"Upload files, Hit Submit button, then ask questions.:blue[- Venkat Reddy]:sunglasses:\")\n",
    "    if user_input:\n",
    "      handle_userinput(user_input)\n",
    "\n",
    "\n",
    "    with st.sidebar:\n",
    "        pdf_docs=st.file_uploader(\"Upload your documents here\" , accept_multiple_files=True)\n",
    "        if st.button(\"Submit\"):\n",
    "          with st.spinner(\"Processing...\"):\n",
    "\n",
    "            # Get PDF Data\n",
    "            raw_text=get_pdf_text(pdf_docs)\n",
    "\n",
    "            #Divide the Data into Chunks\n",
    "            chunked_array=get_text_chunks(raw_text)\n",
    "\n",
    "            #Create embeddings\n",
    "            embeddings=get_embeddings(chunked_array)\n",
    "\n",
    "            #Create vectorstore\n",
    "            vectorstore=FAISS.from_texts(chunked_array,embeddings)\n",
    "\n",
    "            #crate a converstaion chain\n",
    "            st.session_state.convesrational_chain=get_convesrational_chain(vectorstore)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YphGHyMITyNq"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n",
    "#Doesn't work in Chrome sometimes - Due to CSS related issues; Try Forefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.1.5:8501\u001b[0m\n",
      "\u001b[0m\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "Created a chunk of size 3944, which is longer than the specified 1000\n",
      "Created a chunk of size 4012, which is longer than the specified 1000\n",
      "Created a chunk of size 2862, which is longer than the specified 1000\n",
      "Created a chunk of size 1953, which is longer than the specified 1000\n",
      "Created a chunk of size 2077, which is longer than the specified 1000\n",
      "Created a chunk of size 2369, which is longer than the specified 1000\n",
      "Created a chunk of size 2209, which is longer than the specified 1000\n",
      "Created a chunk of size 1732, which is longer than the specified 1000\n",
      "Created a chunk of size 1794, which is longer than the specified 1000\n",
      "Created a chunk of size 3799, which is longer than the specified 1000\n",
      "Created a chunk of size 1791, which is longer than the specified 1000\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:31: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:35: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0)\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:36: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "Created a chunk of size 3944, which is longer than the specified 1000\n",
      "Created a chunk of size 4012, which is longer than the specified 1000\n",
      "Created a chunk of size 2862, which is longer than the specified 1000\n",
      "Created a chunk of size 1953, which is longer than the specified 1000\n",
      "Created a chunk of size 2077, which is longer than the specified 1000\n",
      "Created a chunk of size 2369, which is longer than the specified 1000\n",
      "Created a chunk of size 2209, which is longer than the specified 1000\n",
      "Created a chunk of size 1732, which is longer than the specified 1000\n",
      "Created a chunk of size 1794, which is longer than the specified 1000\n",
      "Created a chunk of size 3799, which is longer than the specified 1000\n",
      "Created a chunk of size 1791, which is longer than the specified 1000\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:46: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = st.session_state.convesrational_chain({\"question\": user_question})\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 08:01:08.263 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.273 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.274 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.275 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-11 08:01:08.275 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.277 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.278 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.279 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.280 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.280 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.282 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-11 08:01:08.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Title of the app\n",
    "st.title(\"BMI Calculator\")\n",
    "\n",
    "# Input: Weight in kilograms\n",
    "weight = st.number_input(\"Enter your weight (kg):\", min_value=0.0, format=\"%.2f\")\n",
    "\n",
    "# Input: Height format selection\n",
    "height_unit = st.radio(\"Select your height unit:\", ['Centimeters', 'Meters', 'Feet'])\n",
    "\n",
    "# Input: Height value based on selected unit\n",
    "height = st.number_input(f\"Enter your height ({height_unit.lower()}):\", min_value=0.0, format=\"%.2f\")\n",
    "\n",
    "# Calculate BMI when button is pressed\n",
    "if st.button(\"Calculate BMI\"):\n",
    "    try:\n",
    "        # Convert height to meters based on selected unit\n",
    "        if height_unit == 'Centimeters':\n",
    "            height_m = height / 100\n",
    "        elif height_unit == 'Feet':\n",
    "            height_m = height / 3.28\n",
    "        else:\n",
    "            height_m = height\n",
    "\n",
    "        # Prevent division by zero\n",
    "        if height_m <= 0:\n",
    "            st.error(\"Height must be greater than zero.\")\n",
    "        else:\n",
    "            bmi = weight / (height_m ** 2)\n",
    "            st.success(f\"Your BMI is {bmi:.2f}\")\n",
    "\n",
    "            # BMI interpretation\n",
    "            if bmi < 16:\n",
    "                st.error(\"You are Extremely Underweight\")\n",
    "            elif 16 <= bmi < 18.5:\n",
    "                st.warning(\"You are Underweight\")\n",
    "            elif 18.5 <= bmi < 25:\n",
    "                st.success(\"You are Healthy\")\n",
    "            elif 25 <= bmi < 30:\n",
    "                st.warning(\"You are Overweight\")\n",
    "            else:\n",
    "                st.error(\"You are Extremely Overweight\")\n",
    "    except:\n",
    "        st.error(\"Please enter valid numeric values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.1.5:8501\u001b[0m\n",
      "\u001b[0m\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.embeddings import OpenAIEmbeddings\n",
      "/Users/giridharsripathi/Documents/training/gpsnew/llm/Topic9_LangChain_Memory/app.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOrSA8/BlF9iLVXjgCIJZsZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
