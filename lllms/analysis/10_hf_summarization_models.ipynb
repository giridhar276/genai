{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1031de",
   "metadata": {},
   "source": [
    "# Hugging Face Summarization: Models that *actually* work (+ correct `pipeline` task names)\n",
    "\n",
    "**Generated:** 2025-09-15 02:52:46\n",
    "\n",
    "This notebook gives you:\n",
    "- A short, practical model list that works well for summarization\n",
    "- The **correct `pipeline` task name** for each (e.g., `\"summarization\"` vs `\"text2text-generation\"`)\n",
    "- Minimal code cells you can run directly\n",
    "- Tips for long documents (chunking / map-reduce)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd1e549",
   "metadata": {},
   "source": [
    "## How to run this notebook\n",
    "\n",
    "1. Make sure you have recent versions of `transformers` (and optionally `accelerate`, `sentencepiece` for T5/PEGASUS/mT5) installed.  \n",
    "2. The first time you run a model, it will download weights from the Hugging Face Hub (internet required).  \n",
    "3. GPU is optional but recommended for speed; otherwise CPU will work but slower.\n",
    "\n",
    "> **Install (optional, run once):**\n",
    "```bash\n",
    "pip install -U transformers accelerate sentencepiece\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206dcd4",
   "metadata": {},
   "source": [
    "## Models by use-case (with correct `pipeline` task)\n",
    "\n",
    "### Short/medium articles (general purpose)\n",
    "- **`facebook/bart-large-cnn`** — task: **`\"summarization\"`**  \n",
    "Reliable “news-style” multi-sentence summaries.\n",
    "- **`sshleifer/distilbart-cnn-12-6`** — task: **`\"summarization\"`**  \n",
    "Faster, smaller BART; great quality/speed tradeoff.\n",
    "- **`google/pegasus-cnn_dailymail`** — task: **`\"summarization\"`**  \n",
    "Strong abstractive summaries in similar domain.\n",
    "\n",
    "### One-sentence / ultra-concise “headline” style\n",
    "- **`google/pegasus-xsum`** — task: **`\"summarization\"`**  \n",
    "Trained for **single-sentence** extreme summaries.\n",
    "\n",
    "### Prompt-steerable (add your own style/instructions)\n",
    "- **`google/flan-t5-base`** / **`google/flan-t5-large`** — task: **`\"text2text-generation\"`**  \n",
    "Use a custom prompt like: *“Summarize in one neutral sentence for an executive.”*  \n",
    "  (T5 also works with `\"summarization\"`, but `\"text2text-generation\"` gives explicit prompt control.)\n",
    "\n",
    "### Long documents (reports, papers, chapters)\n",
    "- **`google/bigbird-pegasus-large-arxiv`** — task: **`\"summarization\"`**  \n",
    "Handles longer inputs better; good for scientific/longer texts.\n",
    "- **`pszemraj/led-large-book-summary`** — task: **`\"summarization\"`**  \n",
    "LED variant tuned for long-form/chapters/book-like content.\n",
    "\n",
    "### Dialogues / meetings / chats\n",
    "- **`google/pegasus-samsum`** — task: **`\"summarization\"`**  \n",
    "Built for conversation transcripts (SAMSum dataset).\n",
    "\n",
    "### Multilingual summarization\n",
    "- **`csebuetnlp/mT5_multilingual_XLSum`** — task: **`\"summarization\"`** *or* **`\"text2text-generation\"`**  \n",
    "Works across many languages; use `\"text2text-generation\"` if you want instruction prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install/upgrade dependencies. Uncomment to run here.\n",
    "# %pip install -U transformers accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▶️ Quick sample texts for testing\n",
    "long_text = (\n",
    "    \"Open-source tools have accelerated AI adoption across industries. \"\n",
    "    \"Teams use models for summarization, search, and code assistance. \"\n",
    "    \"However, deploying models at scale requires attention to latency, cost, and governance. \"\n",
    "    \"In this article, we explore tradeoffs between speed and quality, and offer simple recipes.\"\n",
    ")\n",
    "\n",
    "dialogue_text = (\n",
    "    \"Alice: Hey, did you finish the quarterly report?\\n\"\n",
    "    \"Bob: Almost. I still have to add the revenue projections.\\n\"\n",
    "    \"Alice: Can you share a brief summary for the leadership update?\\n\"\n",
    "    \"Bob: Sure, I'll include a one-paragraph summary and send it by 5 PM.\"\n",
    ")\n",
    "\n",
    "multilingual_text = \"El proyecto avanza según lo previsto, pero necesitamos más pruebas de integración antes del lanzamiento.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7adc8f",
   "metadata": {},
   "source": [
    "### Example 1 — BART (general purpose) · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# News-style multi-sentence summaries\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "result = summarizer(long_text, max_length=180, min_length=60, do_sample=False)\n",
    "print(result[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfd6d9",
   "metadata": {},
   "source": [
    "### Example 2 — DistilBART (faster) · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "print(summarizer(long_text, max_length=150, min_length=50, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c5fba",
   "metadata": {},
   "source": [
    "### Example 3 — PEGASUS CNN/DailyMail · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "print(summarizer(long_text, max_length=180, min_length=60, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214898b",
   "metadata": {},
   "source": [
    "### Example 4 — PEGASUS XSum (single-sentence) · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fe613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "print(summarizer(long_text, max_length=60, min_length=8, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346a24e",
   "metadata": {},
   "source": [
    "### Example 5 — FLAN‑T5 with prompt control · task = `\"text2text-generation\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a custom prompt to steer style/tone/length\n",
    "summarizer = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "PROMPT = \"Summarize in one neutral sentence for an executive:\"\n",
    "out = summarizer(f\"{PROMPT}\\n\\n{long_text}\", max_new_tokens=80)\n",
    "print(out[0][\"generated_text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83cbd3b",
   "metadata": {},
   "source": [
    "### Example 6 — BigBird‑Pegasus for longer inputs · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33249550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from textwrap import wrap\n",
    "\n",
    "# ⚠️ Simple word-based chunking utility (approximate). For production,\n",
    "# consider tokenization-aware chunking to respect model limits.\n",
    "def chunk_text(text, words_per_chunk=250, overlap=30):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = words[i:i+words_per_chunk]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += (words_per_chunk - overlap)\n",
    "    return chunks\n",
    "\n",
    "# Example long-ish input (reuse long_text * N for demonstration)\n",
    "very_long_text = \" \".join([long_text] * 20)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "# Map step: summarize each chunk\n",
    "pieces = chunk_text(very_long_text, words_per_chunk=250, overlap=30)\n",
    "piece_summaries = [summarizer(p, max_length=160, min_length=60, do_sample=False)[0][\"summary_text\"] for p in pieces]\n",
    "\n",
    "# Reduce step: summarize the summaries\n",
    "joined = \" \".join(piece_summaries)\n",
    "final_summary = summarizer(joined, max_length=200, min_length=80, do_sample=False)[0][\"summary_text\"]\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12ddf3",
   "metadata": {},
   "source": [
    "### Example 7 — LED (book-like chapters) · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# LED model tuned for long-form content\n",
    "summarizer = pipeline(\"summarization\", model=\"pszemraj/led-large-book-summary\")\n",
    "\n",
    "# For very long book chapters, chunk first (as above), then map-reduce\n",
    "print(summarizer(long_text, max_length=200, min_length=80, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48425dc4",
   "metadata": {},
   "source": [
    "### Example 8 — SAMSum (dialogues/meetings) · task = `\"summarization\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0750065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"google/pegasus-samsum\")\n",
    "print(summarizer(dialogue_text, max_length=120, min_length=40, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d550348",
   "metadata": {},
   "source": [
    "### Example 9 — mT5 Multilingual · task = `\"summarization\"` *or* `\"text2text-generation\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use 'text2text-generation' if you want to add explicit prompts in various languages\n",
    "summarizer = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "print(summarizer(multilingual_text, max_length=80, min_length=20, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ec1e7",
   "metadata": {},
   "source": [
    "## Quick tips\n",
    "\n",
    "- **Task names to remember**  \n",
    "  - Most ready-to-use summarizers → **`\"summarization\"`**  \n",
    "  - T5/FLAN‑T5 when you want explicit prompts → **`\"text2text-generation\"`**\n",
    "- **Speed vs. quality**  \n",
    "  Start with `sshleifer/distilbart-cnn-12-6` for speed, then try `facebook/bart-large-cnn` for a quality bump.\n",
    "- **Long inputs**  \n",
    "  Use chunking + map-reduce: summarize chunks → join → summarize the summaries. Overlap chunks to reduce boundary loss.\n",
    "- **Deterministic output**  \n",
    "  Set `do_sample=False` (BART/PEGASUS) or keep temperatures low for T5-style models.\n",
    "- **GPU**  \n",
    "  Add `device=0` to the `pipeline(...)` call (if CUDA is available) for significant speedups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7b50b",
   "metadata": {},
   "source": [
    "### Bonus — Generic map-reduce summarization utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ba13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def map_reduce_summarize(text, model_name=\"facebook/bart-large-cnn\",\n",
    "                         words_per_chunk=250, overlap=30,\n",
    "                         map_kwargs=None, reduce_kwargs=None,\n",
    "                         task=\"summarization\"):\n",
    "    \"\"\"Generic chunk->summarize->join->summarize pipeline.\n",
    "    - task: 'summarization' or 'text2text-generation' depending on your model\n",
    "    - map_kwargs/reduce_kwargs: dicts forwarded to pipeline calls\n",
    "    \"\"\"\n",
    "    if map_kwargs is None:\n",
    "        map_kwargs = dict(max_length=160, min_length=60, do_sample=False)\n",
    "    if reduce_kwargs is None:\n",
    "        reduce_kwargs = dict(max_length=200, min_length=80, do_sample=False)\n",
    "\n",
    "    summ = pipeline(task, model=model_name)\n",
    "\n",
    "    # Chunk\n",
    "    def chunk_text(text, words_per_chunk=250, overlap=30):\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            chunk = words[i:i+words_per_chunk]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            i += (words_per_chunk - overlap)\n",
    "        return chunks\n",
    "\n",
    "    pieces = chunk_text(text, words_per_chunk=words_per_chunk, overlap=overlap)\n",
    "\n",
    "    # Map\n",
    "    piece_summaries = [summ(p, **map_kwargs)[0][\"summary_text\"] for p in pieces]\n",
    "\n",
    "    # Reduce\n",
    "    joined = \" \".join(piece_summaries)\n",
    "    final = summ(joined, **reduce_kwargs)[0][\"summary_text\"]\n",
    "    return final\n",
    "\n",
    "# Demo (small text here, but this shines on very long inputs)\n",
    "print(map_reduce_summarize(long_text))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
