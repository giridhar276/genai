{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8ba5396",
      "metadata": {
        "id": "a8ba5396"
      },
      "source": [
        "\n",
        "# Customer Text Summarization — Concurrency + Batching\n",
        "\n",
        "- **Python concurrency**: `ThreadPoolExecutor` processes multiple batches in parallel\n",
        "- **Batch processing**: `llm.batch(prompts)` per chunk\n",
        "- **Single, simple prompt** (no complex fallbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install OpenAI\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install Cohere\n",
        "!pip install langchain-openai langchain-cohere python-dotenv"
      ],
      "metadata": {
        "id": "2MRuIlnaDM2z"
      },
      "id": "2MRuIlnaDM2z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "c6PfGIJiDQ5A"
      },
      "id": "c6PfGIJiDQ5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key"
      ],
      "metadata": {
        "id": "n-wCYoAzDVAC"
      },
      "id": "n-wCYoAzDVAC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "-mlt3jwgDYHF"
      },
      "id": "-mlt3jwgDYHF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cf3f38",
      "metadata": {
        "id": "58cf3f38"
      },
      "outputs": [],
      "source": [
        "\n",
        "CSV_PATH     = \"https://raw.githubusercontent.com/giridhar276/genai/refs/heads/main/datasets/Bank_Customer_conversations.csv\"   # <- set your CSV path\n",
        "TEXT_COL     = \"customer_text\"\n",
        "MODEL        = \"gpt-4o-mini\"\n",
        "TEMPERATURE  = 0\n",
        "TIMEOUT      = 60\n",
        "\n",
        "BATCH_SIZE   = 40      # rows per batch call\n",
        "MAX_WORKERS  = 4       # parallel batches\n",
        "\n",
        "OUTPUT_PATH  = CSV_PATH.replace(\".csv\", \"_with_summary_concurrent.csv\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666e0c3b",
      "metadata": {
        "id": "666e0c3b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Single summarization prompt (simple)\n",
        "PROMPT_TMPL = (\n",
        "    \"Summarize the customer's message in ONE clear sentence focusing on intent/issue and requested action. \"\n",
        "    \"Do not add details that are not present. Return only the summary text on a single line.\\n\\n\"\n",
        "    \"CUSTOMER TEXT:\\n\"\n",
        "    '\\\"\\\"\\\"{text}\\\"\\\"\\\"\\n'\n",
        "    \"Summary:\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3caa1ab3",
      "metadata": {
        "id": "3caa1ab3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found in CSV.\")\n",
        "texts = df[TEXT_COL].astype(str).tolist()\n",
        "len(texts), df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba26ec06",
      "metadata": {
        "id": "ba26ec06"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Model\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE, timeout=TIMEOUT)\n",
        "\n",
        "def chunk_list(seq, size):\n",
        "    for i in range(0, len(seq), size):\n",
        "        yield i, seq[i:i+size]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_batch(texts):\n",
        "    prompts = [PROMPT_TMPL.format(text=t) for t in texts]\n",
        "    responses = llm.batch(prompts)  # keeps order\n",
        "    return [r.content.strip() for r in responses]"
      ],
      "metadata": {
        "id": "GSp8JitsDfk7"
      },
      "id": "GSp8JitsDfk7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03150364",
      "metadata": {
        "id": "03150364"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run batches concurrently\n",
        "chunks = list(chunk_list(texts, BATCH_SIZE))\n",
        "summaries_out = [None] * len(texts)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    fut_map = {ex.submit(summarize_batch, batch): (start, batch) for (start, batch) in chunks}\n",
        "    for fut in as_completed(fut_map):\n",
        "        start, batch = fut_map[fut]\n",
        "        result = fut.result()\n",
        "        summaries_out[start:start+len(batch)] = result\n",
        "        print(f\"Completed rows {start}–{start+len(batch)-1}\")\n",
        "\n",
        "len(summaries_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64fcd85",
      "metadata": {
        "id": "c64fcd85"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save\n",
        "df[\"summary\"] = summaries_out\n",
        "df.to_csv(\"batchprocessing.csv\", index=False)\n",
        "print(f\"Saved: {OUTPUT_PATH}\")\n",
        "df[[TEXT_COL, \"summary\"]].head(10)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}