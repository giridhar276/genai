{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLjfYbAfSCbW"
      },
      "outputs": [],
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install cohere --quiet\n",
        "!pip install langchain_community --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB8kddmfWwoK"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai langchain-cohere python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwoNQTOT_QA"
      },
      "source": [
        "#OpenAI Model - Paid Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTzpLBqhVObK"
      },
      "source": [
        "Get your OpenAI API key here\n",
        "https://platform.openai.com/usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLcmCBxxUDx_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Better way\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIjGxnjpUD53"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI,OpenAI\n",
        "from langchain_cohere import ChatCohere\n",
        "import os\n",
        "llm=OpenAI(temperature=0.9, max_tokens=256)\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)\n",
        "\n",
        "# - temperature: Set to 0.9, which controls the randomness of the output.\n",
        "#   A higher temperature results in more varied and unpredictable outputs,\n",
        "#   while a lower temperature produces more deterministic and conservative outputs.\n",
        "#   This is often used in generative tasks to balance between creativity and relevance.\n",
        "\n",
        "# - max_tokens: Set to 256, which specifies the maximum number of tokens (words or pieces of words)\n",
        "#   that the model can generate in a single response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SrvJiBPUD-v"
      },
      "outputs": [],
      "source": [
        "llm=OpenAI(temperature=0)\n",
        "response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXYuvS5jQ6Rv"
      },
      "source": [
        "#Cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhHtQ_iSQ9EW"
      },
      "source": [
        "Get your Cohere Trail API key here\n",
        "https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEFXkf--Tqn2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Better way\n",
        "os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cZpytJ7QsEw"
      },
      "outputs": [],
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm = ChatCohere(temperature=0.9, max_tokens=256)\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\").content\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSczTG0-V_tY"
      },
      "outputs": [],
      "source": [
        "llm=ChatCohere(temperature=0)\n",
        "response = llm.invoke(\"What is overfitting in Machine Learning? Explain it to a layman\").content\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Aq4hbTgY45z"
      },
      "source": [
        "# Open source models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOi09tmebaTo"
      },
      "source": [
        "\n",
        "* Mistral Model (Mistral 7B, Mixtral8-7B)\n",
        "* LLama (Llam2, Llama3)\n",
        "* Bloom by Hugging Face\n",
        "* Falcon 180B\n",
        "* Opt 175B\n",
        "* Xgen-7B\n",
        "* Vicuna-13B\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZqC43tme-0v"
      },
      "source": [
        "### Top Open-Source Large Language Models for 2024\n",
        "\n",
        "1. **LLaMA 2**:\n",
        "   - Developed by Meta, LLaMA 2 is a generative text model with 7 to 70 billion parameters, fine-tuned with reinforcement learning from human feedback (RLHF).\n",
        "   - Released for research and commercial use in July 2023.\n",
        "   - Includes versions like LLaMA Chat and Code LLaMA for varied natural language tasks.\n",
        "\n",
        "2. **BLOOM**:\n",
        "   - Launched by Hugging Face in 2022, BLOOM is an autoregressive model with 176 billion parameters.\n",
        "   - Supports 46 languages and 13 programming languages.\n",
        "   - Emphasizes transparency and is available for free through Hugging Face.\n",
        "\n",
        "3. **BERT**:\n",
        "   - Introduced by Google in 2018, BERT is known for its bidirectional encoder representations from transformers.\n",
        "   - Achieved state-of-the-art performance in many NLP tasks and is widely used, including in Google Search.\n",
        "\n",
        "4. **Falcon 180B**:\n",
        "   - Released by the Technology Innovation Institute in the UAE in 2023.\n",
        "   - With 180 billion parameters, it rivals models like LLaMA 2 and GPT-3.5.\n",
        "   - Requires significant computing resources.\n",
        "\n",
        "5. **OPT-175B**:\n",
        "   - Part of Meta's suite of pre-trained transformers, released in 2022.\n",
        "   - Ranges from 125M to 175B parameters.\n",
        "   - Available for research use only due to its non-commercial license.\n",
        "\n",
        "6. **XGen-7B**:\n",
        "   - Launched by Salesforce in July 2023, designed for longer context windows.\n",
        "   - Utilizes only 7 billion parameters.\n",
        "   - Available for commercial and research purposes, with some variants under a non-commercial license.\n",
        "\n",
        "7. **GPT-NeoX and GPT-J**:\n",
        "   - Developed by EleutherAI, GPT-NeoX has 20 billion parameters and GPT-J has 6 billion parameters.\n",
        "   - Available for various NLP tasks via the NLP Cloud API.\n",
        "\n",
        "8. **Vicuna-13B**:\n",
        "   - Fine-tuned from LLaMA 13B, Vicuna-13B is a conversational model.\n",
        "   - Performs well in customer service, healthcare, education, and more.\n",
        "   - Achieves high quality, comparable to ChatGPT and Google Bard.\n",
        "\n",
        "### Choosing the Right Open-Source LLM\n",
        "Consider the following factors:\n",
        "- **Purpose**: Ensure the LLM's licensing fits your use case, especially for commercial purposes.\n",
        "- **Necessity**: Evaluate if an LLM is essential for your goals.\n",
        "- **Accuracy**: Larger models typically offer higher accuracy.\n",
        "- **Investment**: Consider the cost of resources for training and operating the LLM.\n",
        "- **Pre-trained Models**: Leverage existing pre-trained models for specific use cases to save resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQO2lLhSWQ0z"
      },
      "source": [
        "#HuggingFace models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NJk5-NNlIZQ"
      },
      "source": [
        "https://huggingface.co/mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8MLcURRcyaZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#Better way\n",
        "from google.colab import userdata\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox_lz4qZYl4h"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3SVQnTIaIhI"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.messages import HumanMessage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynG1WvTVWS-P",
        "outputId": "959b01c6-f8a7-421f-df79-0b14fba209df"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"conversational\",\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "\n",
        "chat = ChatHuggingFace(llm=llm)\n",
        "resp = chat.invoke([HumanMessage(content=\"Write a 4 line poem on AI\")])\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IUaxgdbnOhR"
      },
      "source": [
        "# Llama from Hugging Facehub\n",
        "https://huggingface.co/meta-llama\n",
        "\n",
        "* You need to fill the contact info and wait for the approval.\n",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bJ8n1APujqk",
        "outputId": "6c4cc503-a7be-4cdb-a333-6a06bea93e2e"
      },
      "outputs": [],
      "source": [
        "repo_id=\"meta-llama/Meta-Llama-3.1-8B\"\n",
        "#Throws an error\n",
        "#The model meta-llama/Meta-Llama-3.1-8B is too large to be loaded automatically (16GB > 10GB).\n",
        "#Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n",
        "\n",
        "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"   # fits serverless\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"conversational\",          # chat task matches provider routing\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "\n",
        "chat = ChatHuggingFace(llm=llm)\n",
        "resp = chat.invoke([HumanMessage(content=\"What are some ways to boost creativity?\")])\n",
        "print(resp.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6lV6zlWWVsp"
      },
      "source": [
        "#Replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPL03Yfnr7Vx"
      },
      "source": [
        "- Run and fine-tune open-source models with Replicate's API.https://replicate.com/home\n",
        "- Deploy custom models at scale using one line of code.\n",
        "- Avoid managing infrastructure or learning machine learning details.\n",
        "- Use open-source models or package your own.\n",
        "- Choose to make models public or keep them private.\n",
        "- Start with any open-source model with just one line of code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZMqrpD6sKWy"
      },
      "source": [
        "Replciate API Token\n",
        "\n",
        "On top Left >>> Home>>Click on your id>> API Tokens\n",
        "https://replicate.com/account/api-tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhKCAyesWZlf",
        "outputId": "33f535a5-c5cc-4871-f76c-657d53c4ae7c"
      },
      "outputs": [],
      "source": [
        "!pip install replicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyQu5O3VsOd0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"REPLICATE_API_TOKEN\"] = userdata.get(\"REPLICATE_API_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "g8cuUoGAs9di",
        "outputId": "16234e17-b0a1-4014-e5a8-73be80e84759"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import Replicate\n",
        "\n",
        "replicate_llm = Replicate(\n",
        "    model=\"meta/meta-llama-3.1-405b-instruct\",\n",
        "    model_kwargs={\"temperature\": 0.6},\n",
        ")\n",
        "\n",
        "response = replicate_llm.invoke(\"What are some good strategies for studying?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9G5F5DWTqf"
      },
      "source": [
        "# Groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq-Y3ae0oX7B"
      },
      "source": [
        "* Developed the LPU(Language Processing Unit) chip to run LLMs faster and cheaper.\n",
        "* Offers Groq Cloud to try open-source LLMs like Llama3 or Mixtral.\n",
        "* Allows free use of Llama3 or Mixtral in apps via Groq API Key with rate limits.\n",
        "* Models on Groq https://console.groq.com/docs/models\n",
        "* Get your Groq API key https://console.groq.com/keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilkVBfuso1jK",
        "outputId": "994f12f3-48f8-4d2b-8662-9ef6b886703e"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLICC6MpWVLA"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JCgb4ubpAy6",
        "outputId": "3bbf9c1b-196c-42b8-c8a1-9fc2e196684e"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm=ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\"\n",
        ")\n",
        "result=llm.invoke(\"what are the top 10 quotes about ignorance?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDcBv9MtlU5Y"
      },
      "source": [
        "# Many more ways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr1NfzpUlXY4"
      },
      "source": [
        "https://python.langchain.com/v0.1/docs/integrations/llms/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4JPXjq1eBY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
