{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2f8622",
   "metadata": {},
   "source": [
    "#### Topic modeling automatically finds the main themes (“topics”) in a pile of documents.\n",
    "#### Each topic is a bunch of related words, and each document mixes a few topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2a446",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b3c90c2",
   "metadata": {},
   "source": [
    "### intuition\n",
    "Imagine 1,000 support tickets thrown into one box.\n",
    "\n",
    "The algorithm keeps guessing groups of words that often appear together (e.g., login, sso, token), calls that a topic, and then says each ticket is, say, 60% “Login Issues”, 30% “Network”, 10% “Billing”.\n",
    "\n",
    "That’s it. The rest is how we represent text and which algorithm we use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bca40b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3db50ee3",
   "metadata": {},
   "source": [
    "### the moving parts \n",
    "\n",
    "Document: one email/ticket/review.\n",
    "\n",
    "Token: a word (or bigram like “access token”).\n",
    "\n",
    "Vocabulary: unique tokens across your corpus.\n",
    "\n",
    "Topic: distribution over words (top words help you name the topic).\n",
    "\n",
    "Document–Topic mix: for each document, how much of each topic it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62ea5b",
   "metadata": {},
   "source": [
    "### three families of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2d0b5",
   "metadata": {},
   "source": [
    "##### Classic (bag-of-words)\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) — probabilistic; the textbook baseline.\n",
    "\n",
    "NMF (Non-negative Matrix Factorization) — linear algebra; very practical.\n",
    "\n",
    "Both work on word counts or tf-idf; no deep learning required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297467f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaea66fe",
   "metadata": {},
   "source": [
    "### deep learning flavored\n",
    "\n",
    "Neural Topic Models (NTM) — small neural nets (often VAE-style) that learn topics; variants like ProdLDA, ETM (uses word embeddings), CTM (uses metadata).\n",
    "\n",
    "Conceptually similar to LDA but trained with neural objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba785c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e763f0",
   "metadata": {},
   "source": [
    "### Transformer era (semantic embeddings + clustering)\n",
    "\n",
    "Use sentence/document embeddings (e.g., all-MiniLM-L6-v2), cluster them, then extract top words per cluster.\n",
    "\n",
    "Tools: BERTopic, Top2Vec, or your own “Embeddings + KMeans + keywords”.\n",
    "\n",
    "Often the easiest way to get good, meaningful topics on short texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77f671",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077b34e3",
   "metadata": {},
   "source": [
    "### how to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb73c7",
   "metadata": {},
   "source": [
    "Short texts / slang / mixed language → Embeddings + Clustering (BERTopic).\n",
    "\n",
    "You need simple, explainable math → LDA/NMF.\n",
    "\n",
    "You have metadata (labels/time) & want flexible models → Neural Topic Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ba7fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bbea93d",
   "metadata": {},
   "source": [
    "### evaluation (how to tell if topics are “good”)\n",
    "\n",
    "Topic coherence (C_v, NPMI): do top words in a topic co-occur in real docs?\n",
    "\n",
    "Diversity: do topics avoid repeating the same top words?\n",
    "\n",
    "Human sense-check: can a person name each topic in 5 seconds?\n",
    "\n",
    "Downstream usefulness: dashboards, routing, search improvement, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843acf81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
