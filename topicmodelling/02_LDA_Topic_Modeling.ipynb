{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d645e7",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling (Bag-of-Words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb7667",
   "metadata": {},
   "source": [
    "## Step 0: Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a75067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Bag-of-words vectorizer that turns text into word-count features.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Brings in scikit-learn’s LDA implementation.\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import numpy as np, os, random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b717e7",
   "metadata": {},
   "source": [
    "## Step 1: Load the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r\"topics_100.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b6dee",
   "metadata": {},
   "source": [
    "## Step 2: Counts Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer(...) – Creates a vectorizer with these settings:\n",
    "#stop_words=\"english\" – Drops very common English stopwords (e.g., the, and).\n",
    "#max_features=20000 – Caps vocabulary size; here it’s high and won’t bind for 100 docs.\n",
    "#min_df=2 – Keep only words/bigrams that appear in ≥ 2 documents\n",
    "#ngram_range=(1,2) – Use unigrams and bigrams (e.g., “login”, “login error”).\n",
    "\n",
    "\n",
    "vec = CountVectorizer(stop_words=\"english\", max_features=20000, min_df=2, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "#Learns the vocabulary from text and creates a sparse matrix X of shape (n_docs, vocab_size) with raw counts.\n",
    "X = vec.fit_transform(df[\"text\"].astype(str).tolist())\n",
    "\n",
    "\n",
    "#List of vocabulary strings in the same order as X’s columns.\n",
    "terms = vec.get_feature_names_out()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9528cc",
   "metadata": {},
   "source": [
    "## Step 3: Fit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eff59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_topics = 10 – How many topics to extract\n",
    "n_topics = 6\n",
    "\n",
    "\n",
    "#LDA(...)Configure LDA:\n",
    "#n_components=n_topics – Number of topics.\n",
    "#learning_method=\"batch\" – Full batch variational EM (stable for small/medium data). For very large corpora, try \"online\".\n",
    "#random_state=42 – Reproducibility.\n",
    "#max_iter=50 Max optimization iterations.\n",
    "\n",
    "lda = LDA(n_components=n_topics, learning_method=\"batch\", random_state=42, max_iter=50)\n",
    "\n",
    "#W (docs × topics): how much of each topic per doc\n",
    "#Fits the model to X and returns the document–topic matrix:\n",
    "W = lda.fit_transform(X)\n",
    "\n",
    "#H (topics × words): which words define each topic\n",
    "#The topic–word matrix in counts space:\n",
    "H = lda.components_\n",
    "\n",
    "\n",
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c3b575",
   "metadata": {},
   "source": [
    "## Step 4: Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50180635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_per_topic(H, terms, topn=12):\n",
    "    for k, row in enumerate(H):\n",
    "        top_idx = row.argsort()[-topn:][::-1]\n",
    "        print(f\"LDA Topic {k}: \" + \", \".join(terms[i] for i in top_idx))\n",
    "top_words_per_topic(H, terms, topn=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb9005",
   "metadata": {},
   "source": [
    "## Step 5: Assign & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dominant_topic\"] = W.argmax(axis=1)\n",
    "df.to_csv(\"lda_topics_assigned.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27746a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a74b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
